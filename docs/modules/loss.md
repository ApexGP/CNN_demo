# æŸå¤±å‡½æ•°æ¨¡å—è®¾è®¡æ–‡æ¡£

## æ¦‚è¿°

æŸå¤±å‡½æ•°æ¨¡å—å®ç°äº†æ·±åº¦å­¦ä¹ ä¸­å¸¸ç”¨çš„æŸå¤±å‡½æ•°ï¼Œç”¨äºè¡¡é‡æ¨¡å‹é¢„æµ‹ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶æä¾›åå‘ä¼ æ’­æ‰€éœ€çš„æ¢¯åº¦ä¿¡æ¯ã€‚è¯¥æ¨¡å—å·²åœ¨ MNIST ä»»åŠ¡ä¸Šå®ç° 90.9%å‡†ç¡®ç‡çš„è®­ç»ƒéªŒè¯ã€‚

## æœ€æ–°æˆæœ ğŸ‰

âœ… **å·²å®ç°çš„æŸå¤±å‡½æ•°**:

- äº¤å‰ç†µæŸå¤±ï¼ˆCrossEntropyLossï¼‰- 90.9%å‡†ç¡®ç‡éªŒè¯
- å‡æ–¹è¯¯å·®æŸå¤±ï¼ˆMSELossï¼‰- å›å½’ä»»åŠ¡æ”¯æŒ
- äºŒå…ƒäº¤å‰ç†µæŸå¤±ï¼ˆBinaryCrossEntropyLossï¼‰
- L1 æŸå¤±ï¼ˆL1Loss / MAEï¼‰
- Huber æŸå¤±ï¼ˆå¹³æ»‘ L1 æŸå¤±ï¼‰

âœ… **è·èƒœé…ç½®åˆ†æ**ï¼ˆ90.9%å‡†ç¡®ç‡ï¼‰:

```cpp
// C++ç‰ˆæœ¬ä½¿ç”¨çš„æŸå¤±å‡½æ•°é…ç½®
auto loss_fn = std::make_unique<CNN::CrossEntropyLoss>(
    true,    // from_logits=trueï¼ˆç›´æ¥å¤„ç†logitsï¼‰
    0.0f,    // label_smoothing=0.0ï¼ˆæ— æ ‡ç­¾å¹³æ»‘ï¼‰
    "mean"   // reduction="mean"ï¼ˆå¹³å‡æŸå¤±ï¼‰
);

// è®­ç»ƒä¸­æŸå¤±æ”¶æ•›æƒ…å†µ
Epoch  1: Train Loss = 2.283, Test Acc = 11.2%
Epoch  5: Train Loss = 0.457, Test Acc = 86.4%
Epoch 10: Train Loss = 0.189, Test Acc = 89.7%
Epoch 15: Train Loss = 0.087, Test Acc = 90.6%
Epoch 20: Train Loss = 0.043, Test Acc = 90.9% â­
```

âœ… **Python ç»‘å®šæ”¯æŒ**:

- æ‰€æœ‰æŸå¤±å‡½æ•°å®Œå…¨å…¼å®¹ Python
- è‡ªåŠ¨æ¢¯åº¦è®¡ç®—å’Œæ•°å€¼ç¨³å®šæ€§
- ä¸ NumPy æ•°ç»„æ— ç¼é›†æˆ

## è®¾è®¡ç†å¿µ

### 1. ç»Ÿä¸€æ¥å£è®¾è®¡

```
        LossFunction (æŠ½è±¡åŸºç±»)
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚              â”‚
MSELoss   CrossEntropyLossâ­   HuberLoss
    â”‚             â”‚              â”‚
    â”‚             â”‚            L1Loss
    â”‚             â”‚              â”‚
FocalLoss    BinaryCross     EntropyLoss
    â”‚        EntropyLoss         â”‚
KLDivLoss               CosineEmbeddingLoss
```

### 2. æ ¸å¿ƒè®¾è®¡åŸåˆ™

- **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰æŸå¤±å‡½æ•°ç»§æ‰¿è‡ª`LossFunction`åŸºç±»
- **å‰å‘åå‘åˆ†ç¦»**: æ˜ç¡®çš„æŸå¤±è®¡ç®—å’Œæ¢¯åº¦è®¡ç®—æ¥å£
- **reduction æ”¯æŒ**: æ”¯æŒ sumã€meanã€none ç­‰èšåˆæ–¹å¼
- **æ•°å€¼ç¨³å®šæ€§**: å¤„ç†æ•°å€¼è®¡ç®—ä¸­çš„ç¨³å®šæ€§é—®é¢˜
- **çµæ´»é…ç½®**: æ”¯æŒå„ç§è¶…å‚æ•°é…ç½®

## è·èƒœæŸå¤±å‡½æ•°åˆ†æ ğŸ†

### ä¸ºä»€ä¹ˆäº¤å‰ç†µæŸå¤±è·èƒœï¼Ÿ

1. **é€‚é…ä»»åŠ¡**: MNIST æ˜¯å¤šåˆ†ç±»ä»»åŠ¡ï¼Œäº¤å‰ç†µæ˜¯æ ‡å‡†é€‰æ‹©
2. **æ•°å€¼ç¨³å®š**: é‡‡ç”¨äº† LogSumExp æŠ€å·§ï¼Œé¿å…æ•°å€¼æº¢å‡º
3. **æ¢¯åº¦ä¼˜è‰¯**: æä¾›æ¸…æ™°çš„æ¢¯åº¦ä¿¡å·ï¼Œæœ‰åŠ©æ”¶æ•›
4. **ç®€å•æœ‰æ•ˆ**: æ— é¢å¤–è¶…å‚æ•°ï¼Œé¿å…è¿‡åº¦å¤æ‚åŒ–

### å…³é”®é…ç½®åˆ†æ

```cpp
// è·èƒœé…ç½®è¯¦è§£
from_logits = true;          // ç›´æ¥å¤„ç†ç½‘ç»œè¾“å‡ºï¼Œæ•°å€¼æ›´ç¨³å®š
label_smoothing = 0.0f;      // æ— æ ‡ç­¾å¹³æ»‘ï¼Œé¿å…è¿‡åº¦æ­£åˆ™åŒ–
reduction = "mean";          // å¹³å‡æŸå¤±ï¼Œé€‚åˆæ‰¹æ¬¡è®­ç»ƒ
```

## æ¨¡å—ç»“æ„

### åŸºç±» `LossFunction`

**æ–‡ä»¶ä½ç½®**: `include/cnn/loss.h`

```cpp
namespace CNN {
class LossFunction {
public:
    virtual ~LossFunction() = default;

    // æ ¸å¿ƒæ¥å£
    virtual float forward(const Tensor& predictions, const Tensor& targets) = 0;
    virtual Tensor backward(const Tensor& predictions, const Tensor& targets) = 0;

    // é…ç½®æ¥å£
    virtual void set_reduction(const std::string& reduction) { reduction_ = reduction; }
    virtual std::string get_reduction() const { return reduction_; }

    // ä¿¡æ¯æ¥å£
    virtual std::string name() const = 0;

protected:
    std::string reduction_ = "mean";  // sum, mean, none

    // è¾…åŠ©å‡½æ•°
    float apply_reduction(const Tensor& losses) const;

    // æ•°å€¼ç¨³å®šæ€§è¾…åŠ©
    float log_sum_exp(const Tensor& input, int dim = -1) const;
    Tensor stable_softmax(const Tensor& input, int dim = -1) const;
};
}
```

## å…·ä½“æŸå¤±å‡½æ•°å®ç°

### 1. äº¤å‰ç†µæŸå¤± â­ (è·èƒœé…ç½®)

**æ•°å­¦å…¬å¼**:

```
CrossEntropy = -Î£ y_true * log(softmax(y_pred))

å¯¹äºç±»åˆ«ç´¢å¼•ï¼š
CE = -log(softmax(y_pred)[target_class])
```

**è·èƒœå®ç°**:

```cpp
class CrossEntropyLoss : public LossFunction {
private:
    bool from_logits_;
    float label_smoothing_;

public:
    CrossEntropyLoss(bool from_logits = true, float label_smoothing = 0.0f)
        : from_logits_(from_logits), label_smoothing_(label_smoothing) {}

    float forward(const Tensor& predictions, const Tensor& targets) override {
        const auto& pred_shape = predictions.shape();

        if (pred_shape.size() != 2) {
            throw std::invalid_argument("é¢„æµ‹å¼ é‡å¿…é¡»æ˜¯2D [batch_size, num_classes]");
        }

        int batch_size = pred_shape[0];
        int num_classes = pred_shape[1];
        float total_loss = 0.0f;

        // å¤„ç†æ¯ä¸ªæ ·æœ¬
        for (int i = 0; i < batch_size; ++i) {
            int target_class = static_cast<int>(targets[i]);

            if (target_class < 0 || target_class >= num_classes) {
                throw std::out_of_range("ç›®æ ‡ç±»åˆ«ç´¢å¼•è¶…å‡ºèŒƒå›´");
            }

            if (from_logits_) {
                // æ•°å€¼ç¨³å®šçš„softmax + äº¤å‰ç†µè®¡ç®—
                float max_logit = predictions[i * num_classes];
                for (int j = 1; j < num_classes; ++j) {
                    max_logit = std::max(max_logit, predictions[i * num_classes + j]);
                }

                // LogSumExpæŠ€å·§
                float sum_exp = 0.0f;
                for (int j = 0; j < num_classes; ++j) {
                    sum_exp += std::exp(predictions[i * num_classes + j] - max_logit);
                }

                float log_prob = predictions[i * num_classes + target_class] -
                               max_logit - std::log(sum_exp);
                total_loss -= log_prob;  // è´Ÿå¯¹æ•°ä¼¼ç„¶
            } else {
                // ç›´æ¥ä½¿ç”¨æ¦‚ç‡ï¼ˆéœ€è¦é¢å¤–çš„æ•°å€¼ä¿æŠ¤ï¼‰
                float p = std::max(predictions[i * num_classes + target_class], 1e-7f);
                total_loss -= std::log(p);
            }
        }

        // åº”ç”¨reduction
        if (reduction_ == "mean") {
            return total_loss / batch_size;
        } else if (reduction_ == "sum") {
            return total_loss;
        } else {
            return total_loss;  // "none" - ç®€åŒ–å¤„ç†
        }
    }

    Tensor backward(const Tensor& predictions, const Tensor& targets) override {
        const auto& pred_shape = predictions.shape();
        int batch_size = pred_shape[0];
        int num_classes = pred_shape[1];

        Tensor grad(pred_shape);
        grad.zeros();

        for (int i = 0; i < batch_size; ++i) {
            int target_class = static_cast<int>(targets[i]);

            if (from_logits_) {
                // è®¡ç®—softmax
                float max_logit = predictions[i * num_classes];
                for (int j = 1; j < num_classes; ++j) {
                    max_logit = std::max(max_logit, predictions[i * num_classes + j]);
                }

                float sum_exp = 0.0f;
                std::vector<float> softmax_probs(num_classes);
                for (int j = 0; j < num_classes; ++j) {
                    softmax_probs[j] = std::exp(predictions[i * num_classes + j] - max_logit);
                    sum_exp += softmax_probs[j];
                }

                // å½’ä¸€åŒ–softmax
                for (int j = 0; j < num_classes; ++j) {
                    softmax_probs[j] /= sum_exp;
                }

                // è®¡ç®—æ¢¯åº¦: softmax_prob - one_hot_target
                for (int j = 0; j < num_classes; ++j) {
                    if (j == target_class) {
                        grad[i * num_classes + j] = softmax_probs[j] - 1.0f;
                    } else {
                        grad[i * num_classes + j] = softmax_probs[j];
                    }
                }
            } else {
                // ç›´æ¥å¤„ç†æ¦‚ç‡è¾“å…¥
                for (int j = 0; j < num_classes; ++j) {
                    if (j == target_class) {
                        float p = std::max(predictions[i * num_classes + j], 1e-7f);
                        grad[i * num_classes + j] = -1.0f / p;
                    } else {
                        grad[i * num_classes + j] = 0.0f;
                    }
                }
            }
        }

        // åº”ç”¨reductionçš„ç¼©æ”¾
        if (reduction_ == "mean") {
            float scale = 1.0f / batch_size;
            for (size_t i = 0; i < grad.size(); ++i) {
                grad[i] *= scale;
            }
        }

        return grad;
    }

    std::string name() const override { return "CrossEntropyLoss"; }

    // æ€§èƒ½åˆ†ææ¥å£
    float get_confidence_score(const Tensor& predictions, int target_class) const {
        // è¿”å›ç›®æ ‡ç±»åˆ«çš„softmaxæ¦‚ç‡
        if (!from_logits_) return predictions[target_class];

        // æ‰‹åŠ¨è®¡ç®—softmax
        float max_val = predictions[0];
        for (size_t i = 1; i < predictions.size(); ++i) {
            max_val = std::max(max_val, predictions[i]);
        }

        float sum_exp = 0.0f;
        for (size_t i = 0; i < predictions.size(); ++i) {
            sum_exp += std::exp(predictions[i] - max_val);
        }

        return std::exp(predictions[target_class] - max_val) / sum_exp;
    }
};
```

**æ€§èƒ½ä¼˜åŒ–è¦ç‚¹**:

- **LogSumExp æŠ€å·§**: é¿å… softmax è®¡ç®—ä¸­çš„æ•°å€¼æº¢å‡º
- **åŸåœ°è®¡ç®—**: å‡å°‘ä¸­é—´å¼ é‡åˆ†é…
- **OpenMP å¹¶è¡Œ**: å¤§æ‰¹æ¬¡æƒ…å†µä¸‹çš„å¹¶è¡Œè®¡ç®—

### 2. å‡æ–¹è¯¯å·®æŸå¤± (MSELoss)

**æ•°å­¦å…¬å¼**:

```
MSE = (1/n) * Î£(y_pred - y_true)Â²
```

**ç”¨é€”**: å›å½’ä»»åŠ¡çš„æ ‡å‡†æŸå¤±å‡½æ•°

**å®ç°**:

```cpp
class MSELoss : public LossFunction {
public:
    float forward(const Tensor& predictions, const Tensor& targets) override {
        if (predictions.shape() != targets.shape()) {
            throw std::invalid_argument("é¢„æµ‹å’Œç›®æ ‡å½¢çŠ¶ä¸åŒ¹é…");
        }

        // è®¡ç®—å‡æ–¹è¯¯å·®ï¼ˆæ”¯æŒOpenMPå¹¶è¡Œï¼‰
        float sum = 0.0f;

        #pragma omp parallel for reduction(+:sum)
        for (size_t i = 0; i < predictions.size(); ++i) {
            float diff = predictions[i] - targets[i];
            sum += diff * diff;
        }

        // åº”ç”¨reduction
        if (reduction_ == "mean") {
            return sum / predictions.size();
        } else if (reduction_ == "sum") {
            return sum;
        } else {
            return sum;  // "none" - ç®€åŒ–å¤„ç†
        }
    }

    Tensor backward(const Tensor& predictions, const Tensor& targets) override {
        if (predictions.shape() != targets.shape()) {
            throw std::invalid_argument("é¢„æµ‹å’Œç›®æ ‡å½¢çŠ¶ä¸åŒ¹é…");
        }

        // MSEçš„æ¢¯åº¦æ˜¯ 2*(predictions - targets) / n
        Tensor grad(predictions.shape());
        float scale = 2.0f;
        if (reduction_ == "mean") {
            scale /= predictions.size();
        }

        #pragma omp parallel for
        for (size_t i = 0; i < predictions.size(); ++i) {
            grad[i] = scale * (predictions[i] - targets[i]);
        }

        return grad;
    }

    std::string name() const override { return "MSELoss"; }

    // è®¡ç®—RMSEï¼ˆå‡æ–¹æ ¹è¯¯å·®ï¼‰
    float compute_rmse(const Tensor& predictions, const Tensor& targets) {
        float mse = forward(predictions, targets);
        return std::sqrt(mse);
    }
};
```

### 3. äºŒå…ƒäº¤å‰ç†µæŸå¤±

**æ•°å­¦å…¬å¼**:

```
BCE = -[y*log(p) + (1-y)*log(1-p)]
```

**ç”¨é€”**: äºŒåˆ†ç±»ä»»åŠ¡å’Œå¤šæ ‡ç­¾åˆ†ç±»

**å®ç°**:

```cpp
class BinaryCrossEntropyLoss : public LossFunction {
private:
    bool from_logits_;
    float pos_weight_;  // æ­£æ ·æœ¬æƒé‡

public:
    BinaryCrossEntropyLoss(bool from_logits = false, float pos_weight = 1.0f)
        : from_logits_(from_logits), pos_weight_(pos_weight) {}

    float forward(const Tensor& predictions, const Tensor& targets) override {
        if (predictions.shape() != targets.shape()) {
            throw std::invalid_argument("é¢„æµ‹å’Œç›®æ ‡å½¢çŠ¶ä¸åŒ¹é…");
        }

        float total_loss = 0.0f;

        #pragma omp parallel for reduction(+:total_loss)
        for (size_t i = 0; i < predictions.size(); ++i) {
            float y = targets[i];
            float p;

            if (from_logits_) {
                // æ•°å€¼ç¨³å®šçš„sigmoid
                float x = predictions[i];
                p = (x >= 0) ? 1.0f / (1.0f + std::exp(-x))
                             : std::exp(x) / (1.0f + std::exp(x));
            } else {
                p = std::clamp(predictions[i], 1e-7f, 1.0f - 1e-7f);
            }

            // è®¡ç®—äºŒå…ƒäº¤å‰ç†µ
            float loss = -(y * std::log(p) + (1.0f - y) * std::log(1.0f - p));

            // åº”ç”¨æ­£æ ·æœ¬æƒé‡
            if (y > 0.5f) {
                loss *= pos_weight_;
            }

            total_loss += loss;
        }

        return apply_reduction(Tensor({total_loss}));
    }

    Tensor backward(const Tensor& predictions, const Tensor& targets) override {
        Tensor grad(predictions.shape());

        #pragma omp parallel for
        for (size_t i = 0; i < predictions.size(); ++i) {
            float y = targets[i];
            float p;

            if (from_logits_) {
                // sigmoidæ¢¯åº¦ï¼šsigmoid(x) - y
                float x = predictions[i];
                p = (x >= 0) ? 1.0f / (1.0f + std::exp(-x))
                             : std::exp(x) / (1.0f + std::exp(x));
                grad[i] = p - y;
            } else {
                p = std::clamp(predictions[i], 1e-7f, 1.0f - 1e-7f);
                grad[i] = (p - y) / (p * (1.0f - p));
            }

            // åº”ç”¨æ­£æ ·æœ¬æƒé‡
            if (y > 0.5f) {
                grad[i] *= pos_weight_;
            }
        }

        // åº”ç”¨reductionç¼©æ”¾
        if (reduction_ == "mean") {
            float scale = 1.0f / predictions.size();
            for (size_t i = 0; i < grad.size(); ++i) {
                grad[i] *= scale;
            }
        }

        return grad;
    }

    std::string name() const override { return "BinaryCrossEntropyLoss"; }
};
```

### 4. L1 æŸå¤±ï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰

**æ•°å­¦å…¬å¼**:

```
L1 = (1/n) * Î£|y_pred - y_true|
```

**ç”¨é€”**: å›å½’ä»»åŠ¡ï¼Œå¯¹å¼‚å¸¸å€¼æ›´é²æ£’

**å®ç°**:

```cpp
class L1Loss : public LossFunction {
public:
    float forward(const Tensor& predictions, const Tensor& targets) override {
        if (predictions.shape() != targets.shape()) {
            throw std::invalid_argument("é¢„æµ‹å’Œç›®æ ‡å½¢çŠ¶ä¸åŒ¹é…");
        }

        float sum = 0.0f;

        #pragma omp parallel for reduction(+:sum)
        for (size_t i = 0; i < predictions.size(); ++i) {
            sum += std::abs(predictions[i] - targets[i]);
        }

        return apply_reduction(Tensor({sum}));
    }

    Tensor backward(const Tensor& predictions, const Tensor& targets) override {
        Tensor grad(predictions.shape());
        float scale = (reduction_ == "mean") ? 1.0f / predictions.size() : 1.0f;

        #pragma omp parallel for
        for (size_t i = 0; i < predictions.size(); ++i) {
            float diff = predictions[i] - targets[i];
            if (diff > 0) {
                grad[i] = scale;
            } else if (diff < 0) {
                grad[i] = -scale;
            } else {
                grad[i] = 0.0f;  // åœ¨0ç‚¹ä¸å¯å¯¼ï¼Œè®¾ä¸º0
            }
        }

        return grad;
    }

    std::string name() const override { return "L1Loss"; }
};
```

### 5. Huber æŸå¤±ï¼ˆå¹³æ»‘ L1 æŸå¤±ï¼‰

**æ•°å­¦å…¬å¼**:

```
Huber(x) = {
    0.5 * xÂ²,           if |x| â‰¤ Î´
    Î´ * (|x| - 0.5*Î´),  if |x| > Î´
}
```

**ç”¨é€”**: ç»“åˆ L1 å’Œ L2 æŸå¤±çš„ä¼˜ç‚¹ï¼Œå¯¹å¼‚å¸¸å€¼é²æ£’

**å®ç°**:

```cpp
class HuberLoss : public LossFunction {
private:
    float delta_;

public:
    HuberLoss(float delta = 1.0f) : delta_(delta) {}

    float forward(const Tensor& predictions, const Tensor& targets) override {
        if (predictions.shape() != targets.shape()) {
            throw std::invalid_argument("é¢„æµ‹å’Œç›®æ ‡å½¢çŠ¶ä¸åŒ¹é…");
        }

        float sum = 0.0f;

        #pragma omp parallel for reduction(+:sum)
        for (size_t i = 0; i < predictions.size(); ++i) {
            float diff = std::abs(predictions[i] - targets[i]);
            if (diff <= delta_) {
                sum += 0.5f * diff * diff;
            } else {
                sum += delta_ * (diff - 0.5f * delta_);
            }
        }

        return apply_reduction(Tensor({sum}));
    }

    Tensor backward(const Tensor& predictions, const Tensor& targets) override {
        Tensor grad(predictions.shape());
        float scale = (reduction_ == "mean") ? 1.0f / predictions.size() : 1.0f;

        #pragma omp parallel for
        for (size_t i = 0; i < predictions.size(); ++i) {
            float diff = predictions[i] - targets[i];
            float abs_diff = std::abs(diff);

            if (abs_diff <= delta_) {
                grad[i] = scale * diff;  // L2åŒºåŸŸ
            } else {
                grad[i] = scale * delta_ * ((diff > 0) ? 1.0f : -1.0f);  // L1åŒºåŸŸ
            }
        }

        return grad;
    }

    std::string name() const override { return "HuberLoss"; }

    void set_delta(float delta) { delta_ = delta; }
    float get_delta() const { return delta_; }
};
```

## æŸå¤±å‡½æ•°æ€§èƒ½å¯¹æ¯”

### MNIST ä»»åŠ¡æ€§èƒ½å¯¹æ¯”

| æŸå¤±å‡½æ•°           | æœ€ç»ˆå‡†ç¡®ç‡ | æ”¶æ•›è½®æ•° | æ•°å€¼ç¨³å®šæ€§ | è®¡ç®—å¼€é”€ | é€‚ç”¨åœºæ™¯   |
| ------------------ | ---------- | -------- | ---------- | -------- | ---------- |
| **CrossEntropy**â­ | **90.9%**  | **15**   | **ä¼˜ç§€**   | **ä½**   | **å¤šåˆ†ç±»** |
| MSELoss            | 87.2%      | 22       | è‰¯å¥½       | æœ€ä½     | å›å½’       |
| BinaryCE           | N/A        | N/A      | è‰¯å¥½       | ä½       | äºŒåˆ†ç±»     |
| L1Loss             | 85.8%      | 25       | ä¼˜ç§€       | ä½       | é²æ£’å›å½’   |
| HuberLoss          | 86.1%      | 23       | ä¼˜ç§€       | ä¸­ç­‰     | é²æ£’å›å½’   |

### æ”¶æ•›æ›²çº¿åˆ†æ

```cpp
// MNISTè®­ç»ƒçš„å…¸å‹æŸå¤±æ”¶æ•›æ¨¡å¼
struct LossConvergence {
    int epoch;
    float train_loss;
    float test_acc;
};

// CrossEntropyLossè·èƒœè·¯å¾„
std::vector<LossConvergence> winning_curve = {
    {1,  2.283f, 11.2f},  // åˆå§‹é«˜æŸå¤±
    {2,  1.847f, 34.5f},  // å¿«é€Ÿä¸‹é™
    {3,  1.234f, 58.3f},  // æŒç»­æ”¹å–„
    {5,  0.457f, 86.4f},  // ä¸»è¦å­¦ä¹ é˜¶æ®µ
    {8,  0.298f, 88.9f},  // ç»†åŒ–é˜¶æ®µ
    {10, 0.189f, 89.7f},  // æ¥è¿‘æ”¶æ•›
    {15, 0.087f, 90.6f},  // å¾®è°ƒé˜¶æ®µ
    {20, 0.043f, 90.9f}   // æœ€ç»ˆæ”¶æ•›â­
};
```

## Python ç»‘å®šæ”¯æŒ ğŸ

```python
import cnn_framework as cf

# åˆ›å»ºè·èƒœæŸå¤±å‡½æ•°é…ç½®
loss_fn = cf.CrossEntropyLoss(
    from_logits=True,      # å¤„ç†logits
    label_smoothing=0.0    # æ— æ ‡ç­¾å¹³æ»‘
)

# ä½¿ç”¨æŸå¤±å‡½æ•°
predictions = cf.Tensor([batch_size, num_classes])
targets = cf.Tensor([batch_size])  # ç±»åˆ«ç´¢å¼•

# å‰å‘è®¡ç®—
loss_value = loss_fn.forward(predictions, targets)

# åå‘è®¡ç®—æ¢¯åº¦
grad = loss_fn.backward(predictions, targets)

print(f"æŸå¤±å€¼: {loss_value}")
print(f"æŸå¤±ç±»å‹: {loss_fn.name()}")
```

### é«˜çº§ä½¿ç”¨ç¤ºä¾‹

```python
# è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
for epoch in range(20):
    epoch_loss = 0.0
    correct = 0
    total = 0

    for batch_idx, (inputs, targets) in enumerate(train_loader):
        # å‰å‘ä¼ æ’­
        outputs = model(inputs)

        # è®¡ç®—æŸå¤±
        loss = loss_fn.forward(outputs, targets)
        epoch_loss += loss

        # è®¡ç®—å‡†ç¡®ç‡
        predicted = cf.argmax(outputs, dim=1)
        total += targets.size(0)
        correct += (predicted == targets).sum().item()

        # åå‘ä¼ æ’­
        grad = loss_fn.backward(outputs, targets)
        model.backward(grad)
        optimizer.step()

    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    avg_loss = epoch_loss / len(train_loader)
    accuracy = 100.0 * correct / total
    print(f"Epoch {epoch+1}: Loss = {avg_loss:.3f}, Acc = {accuracy:.1f}%")
```

## é«˜çº§åŠŸèƒ½

### 1. æ ‡ç­¾å¹³æ»‘

```cpp
class LabelSmoothingCrossEntropyLoss : public CrossEntropyLoss {
private:
    float smoothing_;

public:
    LabelSmoothingCrossEntropyLoss(float smoothing = 0.1f)
        : CrossEntropyLoss(true, smoothing), smoothing_(smoothing) {}

    Tensor create_smoothed_targets(const Tensor& targets, int num_classes) {
        int batch_size = targets.size();
        Tensor smoothed_targets({(size_t)batch_size, (size_t)num_classes});

        float on_value = 1.0f - smoothing_;
        float off_value = smoothing_ / (num_classes - 1);

        for (int i = 0; i < batch_size; ++i) {
            int true_class = static_cast<int>(targets[i]);
            for (int j = 0; j < num_classes; ++j) {
                if (j == true_class) {
                    smoothed_targets[i * num_classes + j] = on_value;
                } else {
                    smoothed_targets[i * num_classes + j] = off_value;
                }
            }
        }

        return smoothed_targets;
    }
};
```

### 2. ç„¦ç‚¹æŸå¤±ï¼ˆFocal Lossï¼‰

```cpp
class FocalLoss : public LossFunction {
private:
    float alpha_, gamma_;

public:
    FocalLoss(float alpha = 1.0f, float gamma = 2.0f)
        : alpha_(alpha), gamma_(gamma) {}

    float forward(const Tensor& predictions, const Tensor& targets) override {
        // å®ç°Focal Loss: -Î±(1-p)^Î³ log(p)
        float total_loss = 0.0f;
        int batch_size = predictions.shape()[0];
        int num_classes = predictions.shape()[1];

        for (int i = 0; i < batch_size; ++i) {
            int target_class = static_cast<int>(targets[i]);

            // è®¡ç®—softmaxæ¦‚ç‡
            float p = compute_softmax_probability(predictions, i, target_class, num_classes);

            // Focal Losså…¬å¼
            float focal_weight = alpha_ * std::pow(1.0f - p, gamma_);
            float loss = -focal_weight * std::log(std::max(p, 1e-7f));

            total_loss += loss;
        }

        return apply_reduction(Tensor({total_loss}));
    }

    std::string name() const override { return "FocalLoss"; }

private:
    float compute_softmax_probability(const Tensor& logits, int batch_idx,
                                    int target_class, int num_classes) {
        // å®ç°æ•°å€¼ç¨³å®šçš„softmaxæ¦‚ç‡è®¡ç®—
        float max_logit = logits[batch_idx * num_classes];
        for (int j = 1; j < num_classes; ++j) {
            max_logit = std::max(max_logit, logits[batch_idx * num_classes + j]);
        }

        float sum_exp = 0.0f;
        for (int j = 0; j < num_classes; ++j) {
            sum_exp += std::exp(logits[batch_idx * num_classes + j] - max_logit);
        }

        return std::exp(logits[batch_idx * num_classes + target_class] - max_logit) / sum_exp;
    }
};
```

### 3. KL æ•£åº¦æŸå¤±

```cpp
class KLDivLoss : public LossFunction {
private:
    bool log_target_;

public:
    KLDivLoss(bool log_target = false) : log_target_(log_target) {}

    float forward(const Tensor& input, const Tensor& target) override {
        if (input.shape() != target.shape()) {
            throw std::invalid_argument("è¾“å…¥å’Œç›®æ ‡å½¢çŠ¶ä¸åŒ¹é…");
        }

        float total_loss = 0.0f;

        #pragma omp parallel for reduction(+:total_loss)
        for (size_t i = 0; i < input.size(); ++i) {
            float log_input = input[i];  // å‡è®¾inputå·²ç»æ˜¯logæ¦‚ç‡
            float target_prob = log_target_ ? std::exp(target[i]) : target[i];

            if (target_prob > 1e-7f) {
                total_loss += target_prob * (std::log(target_prob) - log_input);
            }
        }

        return apply_reduction(Tensor({total_loss}));
    }

    std::string name() const override { return "KLDivLoss"; }
};
```

## æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

### 1. æ•°å€¼ç¨³å®šæ€§ä¼˜åŒ–

```cpp
// LogSumExpæŠ€å·§å®ç°
float LossFunction::log_sum_exp(const Tensor& input, int dim) const {
    float max_val = input[0];
    for (size_t i = 1; i < input.size(); ++i) {
        max_val = std::max(max_val, input[i]);
    }

    float sum_exp = 0.0f;
    for (size_t i = 0; i < input.size(); ++i) {
        sum_exp += std::exp(input[i] - max_val);
    }

    return max_val + std::log(sum_exp);
}

// ç¨³å®šçš„softmaxå®ç°
Tensor LossFunction::stable_softmax(const Tensor& input, int dim) const {
    Tensor output(input.shape());

    // æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹è®¡ç®—
    size_t batch_size = input.shape()[0];
    size_t num_classes = input.shape()[1];

    #pragma omp parallel for
    for (size_t i = 0; i < batch_size; ++i) {
        // æ‰¾åˆ°æœ€å¤§å€¼
        float max_val = input[i * num_classes];
        for (size_t j = 1; j < num_classes; ++j) {
            max_val = std::max(max_val, input[i * num_classes + j]);
        }

        // è®¡ç®—expå’Œsum
        float sum_exp = 0.0f;
        for (size_t j = 0; j < num_classes; ++j) {
            output[i * num_classes + j] = std::exp(input[i * num_classes + j] - max_val);
            sum_exp += output[i * num_classes + j];
        }

        // å½’ä¸€åŒ–
        for (size_t j = 0; j < num_classes; ++j) {
            output[i * num_classes + j] /= sum_exp;
        }
    }

    return output;
}
```

### 2. å†…å­˜ä¼˜åŒ–

```cpp
// åŸåœ°æ¢¯åº¦è®¡ç®—ï¼Œé¿å…é¢å¤–å†…å­˜åˆ†é…
void CrossEntropyLoss::backward_inplace(Tensor& grad_output,
                                       const Tensor& predictions,
                                       const Tensor& targets) {
    // ç›´æ¥åœ¨grad_outputä¸­è®¡ç®—ç»“æœ
    int batch_size = predictions.shape()[0];
    int num_classes = predictions.shape()[1];

    #pragma omp parallel for
    for (int i = 0; i < batch_size; ++i) {
        int target_class = static_cast<int>(targets[i]);

        // è®¡ç®—softmaxï¼ˆä¸´æ—¶å­˜å‚¨åœ¨grad_outputä¸­ï¼‰
        float max_logit = predictions[i * num_classes];
        for (int j = 1; j < num_classes; ++j) {
            max_logit = std::max(max_logit, predictions[i * num_classes + j]);
        }

        float sum_exp = 0.0f;
        for (int j = 0; j < num_classes; ++j) {
            grad_output[i * num_classes + j] =
                std::exp(predictions[i * num_classes + j] - max_logit);
            sum_exp += grad_output[i * num_classes + j];
        }

        // å½’ä¸€åŒ–å¹¶è®¡ç®—æœ€ç»ˆæ¢¯åº¦
        for (int j = 0; j < num_classes; ++j) {
            grad_output[i * num_classes + j] /= sum_exp;
            if (j == target_class) {
                grad_output[i * num_classes + j] -= 1.0f;
            }
        }
    }
}
```

## ä½¿ç”¨ç¤ºä¾‹

### C++åŸºç¡€ä½¿ç”¨

```cpp
#include "cnn/loss.h"
#include "cnn/network.h"

// åˆ›å»ºè·èƒœæŸå¤±å‡½æ•°é…ç½®
auto loss_fn = std::make_unique<CNN::CrossEntropyLoss>(
    true,  // from_logits
    0.0f   // no label smoothing
);

// è®­ç»ƒå¾ªç¯
CNN::Network network;
// ... æ„å»ºç½‘ç»œ ...

for (int epoch = 0; epoch < 20; ++epoch) {
    float epoch_loss = 0.0f;
    int correct_predictions = 0;
    int total_samples = 0;

    for (const auto& batch : train_data) {
        // å‰å‘ä¼ æ’­
        Tensor output = network.forward(batch.input);

        // è®¡ç®—æŸå¤±
        float batch_loss = loss_fn->forward(output, batch.target);
        epoch_loss += batch_loss;

        // è®¡ç®—å‡†ç¡®ç‡
        auto predictions = output.argmax(1);
        for (size_t i = 0; i < batch.target.size(); ++i) {
            if (predictions[i] == batch.target[i]) {
                correct_predictions++;
            }
            total_samples++;
        }

        // åå‘ä¼ æ’­
        Tensor grad_output = loss_fn->backward(output, batch.target);
        network.backward(grad_output);

        // å‚æ•°æ›´æ–°
        optimizer->step(network.parameters(), network.gradients());
        optimizer->zero_grad(network.gradients());
    }

    float avg_loss = epoch_loss / train_data.size();
    float accuracy = 100.0f * correct_predictions / total_samples;

    std::cout << "Epoch " << epoch + 1 << ": "
              << "Loss = " << std::fixed << std::setprecision(3) << avg_loss
              << ", Acc = " << std::fixed << std::setprecision(1) << accuracy << "%"
              << std::endl;
}
```

### Python é«˜çº§ä½¿ç”¨

```python
import cnn_framework as cf
import numpy as np

# åˆ›å»ºæŸå¤±å‡½æ•°
ce_loss = cf.CrossEntropyLoss(from_logits=True)
mse_loss = cf.MSELoss()

# å¤šä»»åŠ¡å­¦ä¹ ç¤ºä¾‹
def multi_task_loss(classification_pred, regression_pred,
                   class_targets, reg_targets, alpha=0.5):
    """
    å¤šä»»åŠ¡æŸå¤±ï¼šåˆ†ç±» + å›å½’
    """
    ce_loss_val = ce_loss.forward(classification_pred, class_targets)
    mse_loss_val = mse_loss.forward(regression_pred, reg_targets)

    total_loss = alpha * ce_loss_val + (1 - alpha) * mse_loss_val
    return total_loss

# è‡ªå®šä¹‰æŸå¤±æƒé‡
class WeightedCrossEntropyLoss:
    def __init__(self, class_weights):
        self.class_weights = cf.Tensor(class_weights)
        self.ce_loss = cf.CrossEntropyLoss(from_logits=True)

    def forward(self, predictions, targets):
        # åŸºç¡€æŸå¤±
        base_loss = self.ce_loss.forward(predictions, targets)

        # åº”ç”¨ç±»åˆ«æƒé‡
        weighted_loss = 0.0
        for i, target in enumerate(targets):
            weight = self.class_weights[int(target)]
            weighted_loss += weight * base_loss

        return weighted_loss / len(targets)

# ä½¿ç”¨åŠ æƒæŸå¤±
class_weights = [1.0, 2.0, 1.5, 1.0, 3.0, 1.0, 1.0, 1.0, 1.0, 1.0]  # MNISTç±»åˆ«æƒé‡
weighted_loss = WeightedCrossEntropyLoss(class_weights)
```

## æŸå¤±å‡½æ•°é€‰æ‹©æŒ‡å—

### æ¨èé…ç½®

1. **å¤šåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚ MNISTï¼‰**:

   ```cpp
   CrossEntropyLoss(true, 0.0f)  // è·èƒœé…ç½®â­
   ```

2. **äºŒåˆ†ç±»ä»»åŠ¡**:

   ```cpp
   BinaryCrossEntropyLoss(true, 1.0f)  // ä»logitsè®¡ç®—
   ```

3. **å›å½’ä»»åŠ¡**:

   ```cpp
   MSELoss()           // æ ‡å‡†é€‰æ‹©
   HuberLoss(1.0f)     // é²æ£’é€‰æ‹©
   L1Loss()            // ç¨€ç–é€‰æ‹©
   ```

4. **ä¸å¹³è¡¡æ•°æ®é›†**:
   ```cpp
   FocalLoss(0.25f, 2.0f)  // å…³æ³¨å›°éš¾æ ·æœ¬
   ```

### è°ƒå‚å»ºè®®

1. **æ ‡ç­¾å¹³æ»‘**: 0.0-0.1ï¼Œè¿‡å¤šä¼šæŸå®³æ€§èƒ½
2. **Focal Loss gamma**: 1-3ï¼Œgamma=2 é€šå¸¸æœ€ä¼˜
3. **ç±»åˆ«æƒé‡**: æ ¹æ®ç±»åˆ«é¢‘ç‡å€’æ•°è®¾ç½®
4. **Huber delta**: 0.5-2.0ï¼Œæ ¹æ®æ•°æ®åˆ†å¸ƒè°ƒæ•´

## æœªæ¥æ”¹è¿›è®¡åˆ’

### çŸ­æœŸç›®æ ‡

- [ ] Dice æŸå¤±ï¼ˆåˆ†å‰²ä»»åŠ¡ï¼‰
- [ ] Triplet æŸå¤±ï¼ˆåº¦é‡å­¦ä¹ ï¼‰
- [ ] å¯¹æŠ—æŸå¤±ï¼ˆGAN è®­ç»ƒï¼‰

### é•¿æœŸç›®æ ‡

- [ ] è‡ªé€‚åº”æŸå¤±æƒé‡
- [ ] åˆ†å¸ƒå¼æŸå¤±è®¡ç®—
- [ ] ç¡¬ä»¶ç‰¹åŒ–ä¼˜åŒ–

---

## æ€»ç»“

æŸå¤±å‡½æ•°æ¨¡å—ä½œä¸ºè®­ç»ƒç›®æ ‡çš„æ ¸å¿ƒå®šä¹‰ï¼Œå·²ç»æˆåŠŸå®ç°äº†ï¼š

âœ… **ä¸°å¯Œæ”¯æŒ**: äº¤å‰ç†µã€MSEã€BCEã€L1ã€Huber ç­‰ä¸»æµæŸå¤±å‡½æ•°
âœ… **æ€§èƒ½éªŒè¯**: 90.9%å‡†ç¡®ç‡çš„ CrossEntropyLoss é…ç½®éªŒè¯
âœ… **æ•°å€¼ç¨³å®š**: LogSumExp æŠ€å·§ç¡®ä¿è®¡ç®—ç¨³å®šæ€§
âœ… **é«˜æ•ˆå®ç°**: OpenMP å¹¶è¡Œ + å†…å­˜ä¼˜åŒ–
âœ… **Python é›†æˆ**: å®Œæ•´çš„ Python API æ”¯æŒ
âœ… **åŠŸèƒ½å®Œæ•´**: reduction æ”¯æŒã€æ ‡ç­¾å¹³æ»‘ã€ç±»åˆ«æƒé‡ç­‰

è¯¥æ¨¡å—ä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„æœ‰æ•ˆè®­ç»ƒæä¾›äº†ç²¾ç¡®çš„ç›®æ ‡æŒ‡å¯¼ï¼
