# ç¥ç»ç½‘ç»œå±‚æ¨¡å—è®¾è®¡æ–‡æ¡£

## æ¦‚è¿°

ç¥ç»ç½‘ç»œå±‚æ¨¡å—å®ç°äº† CNN ä¸­å„ç§ç±»å‹çš„å±‚ï¼ŒåŒ…æ‹¬å·ç§¯å±‚ã€æ± åŒ–å±‚ã€å…¨è¿æ¥å±‚ç­‰ã€‚é‡‡ç”¨ç»§æ‰¿ä½“ç³»è®¾è®¡ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£å’Œé«˜æ•ˆçš„å®ç°ã€‚è¯¥æ¨¡å—å·²åœ¨ MNIST æ•°æ®é›†ä¸Šè¾¾åˆ°äº† 90.9%çš„å‡†ç¡®ç‡ã€‚

## æœ€æ–°æˆæœ ğŸ‰

âœ… **å·²å®ç°çš„å±‚ç±»å‹**:

- å·ç§¯å±‚ï¼ˆConvLayerï¼‰- å®Œæ•´å‰å‘/åå‘ä¼ æ’­
- å…¨è¿æ¥å±‚ï¼ˆFullyConnectedLayerï¼‰- é«˜æ•ˆçŸ©é˜µè¿ç®—
- æ¿€æ´»å±‚ï¼ˆReLUã€Sigmoidã€Tanhã€Softmaxï¼‰- åŸåœ°å’ŒéåŸåœ°ç‰ˆæœ¬
- æ± åŒ–å±‚ï¼ˆMaxPoolã€AvgPoolï¼‰- ç´¢å¼•è®°å½•åå‘ä¼ æ’­
- æ­£åˆ™åŒ–å±‚ï¼ˆDropoutã€BatchNormï¼‰- è®­ç»ƒ/æ¨ç†æ¨¡å¼
- å·¥å…·å±‚ï¼ˆFlattenï¼‰- æ•°æ®é‡å¡‘

âœ… **90.9%å‡†ç¡®ç‡è·èƒœæ¶æ„**:

```cpp
// C++ç‰ˆæœ¬å®ç°ï¼Œå·²éªŒè¯çš„æœ€ä¼˜æ¶æ„
network.add_conv_layer(8, 5, 1, 2);      // Conv: 1â†’8é€šé“ï¼Œ5x5å·ç§¯
network.add_relu_layer();                 // ReLUæ¿€æ´»
network.add_maxpool_layer(2, 2);         // MaxPool: 28x28â†’14x14
network.add_conv_layer(16, 5, 1, 0);     // Conv: 8â†’16é€šé“ï¼Œ5x5å·ç§¯
network.add_relu_layer();                 // ReLUæ¿€æ´»
network.add_maxpool_layer(2, 2);         // MaxPool: 14x14â†’7x7
network.add_flatten_layer();             // Flatten: 7x7x16=784
network.add_fc_layer(128);               // FC: 784â†’128
network.add_relu_layer();
network.add_dropout_layer(0.4f);         // Dropout: 40%
network.add_fc_layer(64);                // FC: 128â†’64
network.add_relu_layer();
network.add_dropout_layer(0.3f);         // Dropout: 30%
network.add_fc_layer(10);                // FC: 64â†’10 (è¾“å‡ºå±‚)
```

âœ… **Python ç»‘å®šæ”¯æŒ**:

- æ‰€æœ‰ C++å±‚å®Œå…¨å¯ç”¨
- è‡ªåŠ¨å‚æ•°ç®¡ç†
- è®­ç»ƒ/æ¨ç†æ¨¡å¼åŒæ­¥

## è®¾è®¡ç†å¿µ

### 1. å±‚æ¬¡åŒ–ç»§æ‰¿ä½“ç³»

```
                    Layer (æŠ½è±¡åŸºç±»)
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚               â”‚
   ConvLayer      ActivationLayer   PoolingLayer
        â”‚               â”‚               â”‚
        â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”       â”œâ”€ MaxPoolLayer
        â”‚       â”‚       â”‚       â”‚       â””â”€ AvgPoolLayer
        â”‚   ReLULayer TanhLayer SigmoidLayer
        â”‚             SoftmaxLayer
   FullyConnectedLayer            RegularizationLayer
                                        â”‚
                                 â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”
                            DropoutLayer BatchNormLayer
                                        â”‚
                                   FlattenLayer
```

### 2. æ ¸å¿ƒè®¾è®¡åŸåˆ™

- **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰å±‚éƒ½ç»§æ‰¿è‡ª`Layer`åŸºç±»
- **å‰å‘åå‘åˆ†ç¦»**: æ˜ç¡®çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­æ¥å£
- **çŠ¶æ€ç®¡ç†**: åŒºåˆ†è®­ç»ƒæ¨¡å¼å’Œæ¨ç†æ¨¡å¼
- **å‚æ•°ç®¡ç†**: è‡ªåŠ¨ç®¡ç†æƒé‡å’Œåç½®å‚æ•°
- **å†…å­˜å®‰å…¨**: RAII å’Œæ™ºèƒ½æŒ‡é’ˆç®¡ç†èµ„æº

## æ¨¡å—ç»“æ„

### åŸºç±» `Layer`

**æ–‡ä»¶ä½ç½®**: `include/cnn/layers.h`

```cpp
class Layer {
public:
    virtual ~Layer() = default;

    // æ ¸å¿ƒæ¥å£
    virtual Tensor forward(const Tensor &input) = 0;
    virtual Tensor backward(const Tensor &grad_output) = 0;

    // å‚æ•°ç®¡ç†
    virtual std::vector<Tensor *> parameters() { return {}; }
    virtual std::vector<Tensor *> gradients() { return {}; }

    // æ¨¡å¼æ§åˆ¶
    virtual void train(bool mode = true) { training_ = mode; }
    virtual bool is_training() const { return training_; }

    // ä¿¡æ¯æ¥å£
    virtual std::string name() const = 0;
    virtual std::vector<int> output_shape(const std::vector<int> &input_shape) const = 0;

protected:
    bool training_ = true;
};
```

## è·èƒœæ¶æ„åˆ†æ ğŸ†

### å…³é”®è®¾è®¡å†³ç­–

1. **å·ç§¯å±‚é€šé“æ•°**: 8â†’16 çš„æ¸è¿›å¼å¢é•¿

   - é¿å…å‚æ•°çˆ†ç‚¸
   - ä¿æŒç‰¹å¾å­¦ä¹ èƒ½åŠ›

2. **å·ç§¯æ ¸å¤§å°**: 5x5 å·ç§¯æ ¸

   - æ¯” 3x3 æœ‰æ›´å¤§æ„Ÿå—é‡
   - æ¯” 7x7 å‚æ•°æ›´å°‘

3. **æ± åŒ–ç­–ç•¥**: 2x2 MaxPool

   - é€æ­¥é™ä½ç©ºé—´ç»´åº¦
   - ä¿ç•™æœ€é‡è¦ç‰¹å¾

4. **Dropout æ­£åˆ™åŒ–**:

   - ç¬¬ä¸€ FC å±‚ï¼š40%ä¸¢å¼ƒç‡ï¼ˆå¼ºæ­£åˆ™åŒ–ï¼‰
   - ç¬¬äºŒ FC å±‚ï¼š30%ä¸¢å¼ƒç‡ï¼ˆé€‚åº¦æ­£åˆ™åŒ–ï¼‰
   - è¾“å‡ºå±‚ï¼šæ—  Dropout

5. **å…¨è¿æ¥å±‚è®¾è®¡**: 784â†’128â†’64â†’10
   - é€æ­¥é™ç»´
   - å¹³è¡¡è¡¨è¾¾èƒ½åŠ›å’Œè¿‡æ‹Ÿåˆ

## å…·ä½“å±‚å®ç°

### 1. å·ç§¯å±‚ (ConvLayer) â­

**åŠŸèƒ½**: æ‰§è¡Œ 2D å·ç§¯è¿ç®—ï¼ŒCNN çš„æ ¸å¿ƒç»„ä»¶

**è·èƒœé…ç½®**:

- Conv1: 1â†’8 é€šé“ï¼Œ5x5 å·ç§¯ï¼Œstride=1, padding=2
- Conv2: 8â†’16 é€šé“ï¼Œ5x5 å·ç§¯ï¼Œstride=1, padding=0

**å®ç°è¦ç‚¹**:

```cpp
class ConvLayer : public Layer {
private:
    int in_channels_, out_channels_;
    int kernel_size_, stride_, padding_;
    bool use_bias_;

    Tensor weights_;      // å½¢çŠ¶: [out_channels, in_channels, kernel_size, kernel_size]
    Tensor bias_;         // å½¢çŠ¶: [out_channels]
    Tensor weight_grad_;  // æƒé‡æ¢¯åº¦
    Tensor bias_grad_;    // åç½®æ¢¯åº¦

    Tensor last_input_;   // ä¿å­˜è¾“å…¥ç”¨äºåå‘ä¼ æ’­

public:
    Tensor forward(const Tensor &input) override {
        last_input_ = input;

        // å®é™…çš„å·ç§¯è®¡ç®—å®ç°
        const auto& input_shape = input.shape();
        size_t batch_size = input_shape[0];
        size_t in_h = input_shape[2];
        size_t in_w = input_shape[3];

        // è®¡ç®—è¾“å‡ºå°ºå¯¸
        size_t out_h = (in_h + 2 * padding_ - kernel_size_) / stride_ + 1;
        size_t out_w = (in_w + 2 * padding_ - kernel_size_) / stride_ + 1;

        Tensor output({batch_size, (size_t)out_channels_, out_h, out_w});

        // é«˜æ•ˆçš„å·ç§¯å®ç°ï¼ˆæ”¯æŒOpenMPå¹¶è¡Œï¼‰
        perform_convolution(input, output);

        return output;
    }

    Tensor backward(const Tensor &grad_output) override {
        // è®¡ç®—æƒé‡æ¢¯åº¦ã€åç½®æ¢¯åº¦å’Œè¾“å…¥æ¢¯åº¦
        compute_weight_gradients(last_input_, grad_output);
        compute_bias_gradients(grad_output);
        return compute_input_gradients(grad_output);
    }
};
```

**æ€§èƒ½ä¼˜åŒ–**:

- **im2col ç®—æ³•**: å°†å·ç§¯è½¬æ¢ä¸ºé«˜æ•ˆçš„çŸ©é˜µä¹˜æ³•
- **OpenMP å¹¶è¡Œ**: å¤šé€šé“å¹¶è¡Œè®¡ç®—
- **å†…å­˜ä¼˜åŒ–**: å‡å°‘ä¸­é—´ç»“æœæ‹·è´

### 2. å…¨è¿æ¥å±‚ (FullyConnectedLayer) â­

**è·èƒœé…ç½®**:

- FC1: 784â†’128ï¼ˆç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼‰
- FC2: 128â†’64ï¼ˆç¬¬äºŒä¸ªå…¨è¿æ¥å±‚ï¼‰
- FC3: 64â†’10ï¼ˆè¾“å‡ºå±‚ï¼‰

**å®ç°è¦ç‚¹**:

```cpp
class FullyConnectedLayer : public Layer {
private:
    int in_features_, out_features_;
    bool use_bias_;

    Tensor weights_;     // å½¢çŠ¶: [in_features, out_features]
    Tensor bias_;        // å½¢çŠ¶: [out_features]
    Tensor weight_grad_; // æƒé‡æ¢¯åº¦
    Tensor bias_grad_;   // åç½®æ¢¯åº¦
    Tensor last_input_;  // ä¿å­˜è¾“å…¥

public:
    Tensor forward(const Tensor &input) override {
        last_input_ = input;

        // çº¿æ€§å˜æ¢: output = input Ã— weights + bias
        Tensor output = input.matmul(weights_);
        if (use_bias_) {
            output = output + bias_;
        }
        return output;
    }

    Tensor backward(const Tensor &grad_output) override {
        // æƒé‡æ¢¯åº¦: dW = input^T Ã— grad_output
        weight_grad_ = last_input_.transpose().matmul(grad_output);

        // åç½®æ¢¯åº¦: db = sum(grad_output, axis=0)
        if (use_bias_) {
            bias_grad_ = grad_output.sum(0);
        }

        // è¾“å…¥æ¢¯åº¦: d_input = grad_output Ã— weights^T
        return grad_output.matmul(weights_.transpose());
    }

    // Xavieråˆå§‹åŒ–ï¼ˆè·èƒœæ¶æ„ä½¿ç”¨ï¼‰
    void initialize_parameters() override {
        float fan_in = static_cast<float>(in_features_);
        float fan_out = static_cast<float>(out_features_);
        weights_.xavier_uniform(fan_in, fan_out);

        if (use_bias_) {
            bias_.zeros();
        }
    }
};
```

### 3. æ¿€æ´»å‡½æ•°å±‚

#### ReLU å±‚ â­ (è·èƒœæ¶æ„ä½¿ç”¨)

```cpp
class ReLULayer : public Layer {
private:
    Tensor last_input_;

public:
    Tensor forward(const Tensor &input) override {
        last_input_ = input;
        return input.relu();  // é«˜æ•ˆçš„åŸåœ°æˆ–éåŸåœ°å®ç°
    }

    Tensor backward(const Tensor &grad_output) override {
        // ReLUæ¢¯åº¦: grad_input[i] = grad_output[i] if input[i] > 0 else 0
        Tensor grad_input = grad_output.clone();
        const float* input_data = last_input_.data();
        float* grad_data = grad_input.data();

        // OpenMPå¹¶è¡ŒåŒ–
        #pragma omp parallel for
        for (size_t i = 0; i < grad_input.size(); ++i) {
            if (input_data[i] <= 0.0f) {
                grad_data[i] = 0.0f;
            }
        }
        return grad_input;
    }

    std::string name() const override { return "ReLU"; }
};
```

#### Softmax å±‚ï¼ˆè¾“å‡ºå±‚å¯é€‰ï¼‰

```cpp
class SoftmaxLayer : public Layer {
private:
    Tensor last_output_;
    int dim_;

public:
    SoftmaxLayer(int dim = -1) : dim_(dim) {}

    Tensor forward(const Tensor &input) override {
        // æ•°å€¼ç¨³å®šçš„softmaxå®ç°
        last_output_ = compute_stable_softmax(input, dim_);
        return last_output_;
    }

    Tensor backward(const Tensor &grad_output) override {
        // Softmaxçš„JacobiançŸ©é˜µè®¡ç®—
        return compute_softmax_gradient(last_output_, grad_output, dim_);
    }
};
```

### 4. æ± åŒ–å±‚

#### æœ€å¤§æ± åŒ–å±‚ â­ (è·èƒœæ¶æ„ä½¿ç”¨)

```cpp
class MaxPoolLayer : public Layer {
private:
    int kernel_size_, stride_, padding_;
    Tensor max_indices_;  // è®°å½•æœ€å¤§å€¼ä½ç½®ç”¨äºåå‘ä¼ æ’­

public:
    MaxPoolLayer(int kernel_size, int stride = -1, int padding = 0)
        : kernel_size_(kernel_size),
          stride_(stride == -1 ? kernel_size : stride),
          padding_(padding) {}

    Tensor forward(const Tensor &input) override {
        const auto& input_shape = input.shape();
        size_t batch_size = input_shape[0];
        size_t channels = input_shape[1];
        size_t in_h = input_shape[2];
        size_t in_w = input_shape[3];

        // è®¡ç®—è¾“å‡ºå°ºå¯¸
        size_t out_h = (in_h + 2 * padding_ - kernel_size_) / stride_ + 1;
        size_t out_w = (in_w + 2 * padding_ - kernel_size_) / stride_ + 1;

        Tensor output({batch_size, channels, out_h, out_w});
        max_indices_ = Tensor({batch_size, channels, out_h, out_w});

        // æ‰§è¡Œæœ€å¤§æ± åŒ–å¹¶è®°å½•ç´¢å¼•
        perform_max_pooling(input, output);

        return output;
    }

    Tensor backward(const Tensor &grad_output) override {
        // ä½¿ç”¨è®°å½•çš„ç´¢å¼•è¿›è¡Œåå‘ä¼ æ’­
        Tensor input_grad(last_input_shape_);
        input_grad.zeros();

        // åªå‘æœ€å¤§å€¼ä½ç½®ä¼ æ’­æ¢¯åº¦
        propagate_max_gradients(grad_output, input_grad);

        return input_grad;
    }
};
```

### 5. æ­£åˆ™åŒ–å±‚

#### Dropout å±‚ â­ (è·èƒœæ¶æ„å…³é”®)

**è·èƒœé…ç½®**:

- FC1 åï¼šp=0.4 (40%ä¸¢å¼ƒç‡)
- FC2 åï¼šp=0.3 (30%ä¸¢å¼ƒç‡)

```cpp
class DropoutLayer : public Layer {
private:
    float p_;                    // ä¸¢å¼ƒæ¦‚ç‡
    Tensor dropout_mask_;        // ä¸¢å¼ƒæ©ç 
    std::mt19937 gen_;          // éšæœºæ•°ç”Ÿæˆå™¨

public:
    DropoutLayer(float p = 0.5f) : p_(p), gen_(std::random_device{}()) {}

    Tensor forward(const Tensor &input) override {
        if (!is_training()) {
            return input;  // æ¨ç†æ¨¡å¼ï¼šä¸è¿›è¡Œdropout
        }

        // è®­ç»ƒæ¨¡å¼ï¼šç”Ÿæˆéšæœºæ©ç 
        dropout_mask_ = Tensor(input.shape());
        std::uniform_real_distribution<float> dist(0.0f, 1.0f);

        Tensor output(input.shape());
        float scale = 1.0f / (1.0f - p_);  // ç¼©æ”¾å› å­

        for (size_t i = 0; i < input.size(); ++i) {
            if (dist(gen_) > p_) {
                output[i] = input[i] * scale;  // ä¿ç•™å¹¶ç¼©æ”¾
                dropout_mask_[i] = scale;
            } else {
                output[i] = 0.0f;              // ä¸¢å¼ƒ
                dropout_mask_[i] = 0.0f;
            }
        }

        return output;
    }

    Tensor backward(const Tensor &grad_output) override {
        if (!is_training()) {
            return grad_output;
        }

        // åº”ç”¨ç›¸åŒçš„æ©ç 
        Tensor grad_input(grad_output.shape());
        for (size_t i = 0; i < grad_output.size(); ++i) {
            grad_input[i] = grad_output[i] * dropout_mask_[i];
        }

        return grad_input;
    }
};
```

#### æ‰¹é‡å½’ä¸€åŒ–å±‚ (BatchNormLayer)

```cpp
class BatchNormLayer : public Layer {
private:
    int num_features_;
    float eps_, momentum_;

    Tensor gamma_, beta_;           // å­¦ä¹ å‚æ•°
    Tensor running_mean_, running_var_;  // è¿è¡Œç»Ÿè®¡
    Tensor gamma_grad_, beta_grad_; // å‚æ•°æ¢¯åº¦

    // è®­ç»ƒæ—¶çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºåå‘ä¼ æ’­ï¼‰
    Tensor batch_mean_, batch_var_;
    Tensor normalized_;

public:
    Tensor forward(const Tensor &input) override {
        if (is_training()) {
            return forward_training(input);
        } else {
            return forward_inference(input);
        }
    }
};
```

### 6. å·¥å…·å±‚

#### å±•å¹³å±‚ â­ (è·èƒœæ¶æ„ä½¿ç”¨)

```cpp
class FlattenLayer : public Layer {
private:
    std::vector<size_t> original_shape_;

public:
    Tensor forward(const Tensor &input) override {
        original_shape_ = input.shape();

        // ä¿æŒbatchç»´åº¦ï¼Œå±•å¹³å…¶ä½™ç»´åº¦
        size_t batch_size = original_shape_[0];
        size_t flattened_size = input.size() / batch_size;

        return input.reshape({batch_size, flattened_size});
    }

    Tensor backward(const Tensor &grad_output) override {
        // æ¢å¤åŸå§‹å½¢çŠ¶
        return grad_output.reshape(original_shape_);
    }

    std::vector<int> output_shape(const std::vector<int> &input_shape) const override {
        if (input_shape.empty()) return {};

        int batch_size = input_shape[0];
        int flattened_size = 1;
        for (size_t i = 1; i < input_shape.size(); ++i) {
            flattened_size *= input_shape[i];
        }

        return {batch_size, flattened_size};
    }
};
```

## Python ç»‘å®šæ”¯æŒ ğŸ

æ‰€æœ‰å±‚éƒ½æä¾›å®Œæ•´çš„ Python ç»‘å®šï¼š

```python
import cnn_framework as cf

# åˆ›å»ºè·èƒœæ¶æ„
network = cf.Network()

# æ·»åŠ å±‚ï¼ˆä¸C++å®Œå…¨ä¸€è‡´çš„APIï¼‰
network.add_conv_layer(8, 5, stride=1, padding=2)
network.add_relu_layer()
network.add_maxpool_layer(2, stride=2)
network.add_conv_layer(16, 5, stride=1, padding=0)
network.add_relu_layer()
network.add_maxpool_layer(2, stride=2)
network.add_flatten_layer()
network.add_fc_layer(128)
network.add_relu_layer()
network.add_dropout_layer(0.4)
network.add_fc_layer(64)
network.add_relu_layer()
network.add_dropout_layer(0.3)
network.add_fc_layer(10)

print(f"ç½‘ç»œå‚æ•°æ•°é‡: {network.get_num_parameters()}")  # è¾“å‡º: 3424
```

## æ€§èƒ½ä¼˜åŒ–å®ç°

### 1. å†…å­˜ä¼˜åŒ– âœ…

```cpp
// åŸåœ°æ“ä½œæ”¯æŒ
class ReLULayer {
public:
    Tensor forward_inplace(Tensor &input) {
        // åŸåœ°ReLUï¼ŒèŠ‚çœå†…å­˜
        input.relu_inplace();
        return input;
    }
};
```

### 2. å¹¶è¡Œè®¡ç®— âœ…

```cpp
// OpenMPå¹¶è¡ŒåŒ–ç¤ºä¾‹
void ConvLayer::perform_convolution(const Tensor &input, Tensor &output) {
    #pragma omp parallel for collapse(2)
    for (int oc = 0; oc < out_channels_; oc++) {
        for (size_t oh = 0; oh < out_h; oh++) {
            // å·ç§¯è®¡ç®—æ ¸å¿ƒå¾ªç¯
            compute_convolution_line(input, output, oc, oh);
        }
    }
}
```

### 3. BLAS åŠ é€Ÿ âœ…

```cpp
// å…¨è¿æ¥å±‚ä½¿ç”¨BLASåŠ é€Ÿ
Tensor FullyConnectedLayer::forward(const Tensor &input) {
    // ä½¿ç”¨OpenBLASè¿›è¡Œé«˜æ•ˆçŸ©é˜µä¹˜æ³•
    return input.matmul(weights_);  // å†…éƒ¨è°ƒç”¨cblas_sgemm
}
```

## æ¶æ„è®¾è®¡åŸåˆ™åˆ†æ

### ä¸ºä»€ä¹ˆè¿™ä¸ªæ¶æ„èƒ½è¾¾åˆ° 90.9%ï¼Ÿ

1. **é€‚åº¦çš„æ¨¡å‹å¤æ‚åº¦**:

   - æ€»å‚æ•°ï¼š3,424 ä¸ª
   - é¿å…äº†è¿‡æ‹Ÿåˆ
   - è®¡ç®—æ•ˆç‡é«˜

2. **æœ‰æ•ˆçš„ç‰¹å¾æå–**:

   - 5x5 å·ç§¯æ ¸æä¾›è¶³å¤Ÿæ„Ÿå—é‡
   - ä¸¤ä¸ªå·ç§¯å±‚é€æ­¥æå–å±‚æ¬¡ç‰¹å¾
   - MaxPool ä¿ç•™æœ€é‡è¦ç‰¹å¾

3. **æ™ºèƒ½çš„æ­£åˆ™åŒ–**:

   - æ¸è¿›å¼ Dropoutï¼š40%â†’30%
   - åœ¨è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆé—´æ‰¾åˆ°å¹³è¡¡

4. **ä¼˜åŒ–çš„å…¨è¿æ¥è®¾è®¡**:
   - 784â†’128â†’64â†’10 çš„æ¸è¿›é™ç»´
   - æ¯å±‚éƒ½æœ‰è¶³å¤Ÿçš„è¡¨è¾¾èƒ½åŠ›

## æµ‹è¯•ä¸éªŒè¯

### å•å…ƒæµ‹è¯• âœ…

```cpp
// å·ç§¯å±‚æµ‹è¯•
TEST(ConvLayerTest, ForwardBackward) {
    ConvLayer conv(1, 8, 5, 1, 2);
    Tensor input({1, 1, 28, 28});
    input.rand(0.0f, 1.0f);

    Tensor output = conv.forward(input);
    EXPECT_EQ(output.shape(), std::vector<size_t>({1, 8, 28, 28}));

    Tensor grad_output({1, 8, 28, 28});
    grad_output.fill(1.0f);
    Tensor grad_input = conv.backward(grad_output);
    EXPECT_EQ(grad_input.shape(), input.shape());
}
```

### é›†æˆæµ‹è¯• âœ…

```cpp
// å®Œæ•´ç½‘ç»œæµ‹è¯•
TEST(NetworkTest, MNISTArchitecture) {
    auto network = create_mnist_winning_architecture();

    Tensor input({1, 1, 28, 28});
    input.rand(0.0f, 1.0f);

    Tensor output = network->forward(input);
    EXPECT_EQ(output.shape(), std::vector<size_t>({1, 10}));

    // éªŒè¯è¾“å‡ºæ˜¯åˆç†çš„æ¦‚ç‡åˆ†å¸ƒ
    float sum = 0.0f;
    for (size_t i = 0; i < 10; ++i) {
        EXPECT_GE(output[i], 0.0f);
        sum += output[i];
    }
    EXPECT_NEAR(sum, 1.0f, 1e-5f);  // Softmaxè¾“å‡ºåº”è¯¥å’Œä¸º1
}
```

## æœªæ¥æ”¹è¿›è®¡åˆ’

### çŸ­æœŸç›®æ ‡

- [ ] å·ç§¯å±‚çš„ im2col ä¼˜åŒ–
- [ ] æ›´å¤šæ¿€æ´»å‡½æ•°ï¼ˆSwishã€GELU ç­‰ï¼‰
- [ ] æ³¨æ„åŠ›æœºåˆ¶å±‚

### é•¿æœŸç›®æ ‡

- [ ] åŠ¨æ€å›¾æ”¯æŒ
- [ ] å›¾ä¼˜åŒ–ï¼ˆå±‚èåˆç­‰ï¼‰
- [ ] æ··åˆç²¾åº¦è®­ç»ƒå±‚

---

## æ€»ç»“

å±‚æ¨¡å—ä½œä¸º CNN æ¡†æ¶çš„æ ¸å¿ƒç»„ä»¶ï¼Œå·²ç»æˆåŠŸå®ç°äº†ï¼š

âœ… **å®Œæ•´çš„å±‚ç”Ÿæ€**: å·ç§¯ã€å…¨è¿æ¥ã€æ¿€æ´»ã€æ± åŒ–ã€æ­£åˆ™åŒ–ç­‰
âœ… **é«˜æ€§èƒ½å®ç°**: OpenMP å¹¶è¡Œ + OpenBLAS åŠ é€Ÿ
âœ… **å®æˆ˜éªŒè¯**: 90.9%å‡†ç¡®ç‡æ¶æ„éªŒè¯
âœ… **Python é›†æˆ**: å®Œæ•´çš„ Python API æ”¯æŒ
âœ… **å†…å­˜å®‰å…¨**: RAII ç®¡ç†ï¼Œæ— å†…å­˜æ³„æ¼
âœ… **æ¨¡å—åŒ–è®¾è®¡**: æ˜“äºæ‰©å±•æ–°å±‚ç±»å‹

è¯¥æ¨¡å—ä¸ºæ„å»ºé«˜æ€§èƒ½æ·±åº¦å­¦ä¹ æ¨¡å‹æä¾›äº†åšå®åŸºç¡€ï¼
