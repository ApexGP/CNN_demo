# ä¼˜åŒ–å™¨æ¨¡å—è®¾è®¡æ–‡æ¡£

## æ¦‚è¿°

ä¼˜åŒ–å™¨æ¨¡å—å®ç°äº†æ·±åº¦å­¦ä¹ ä¸­å¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•ï¼Œè´Ÿè´£æ ¹æ®æ¢¯åº¦ä¿¡æ¯æ›´æ–°ç½‘ç»œå‚æ•°ã€‚é‡‡ç”¨ç­–ç•¥æ¨¡å¼è®¾è®¡ï¼Œæ”¯æŒå¤šç§ä¼˜åŒ–ç®—æ³•ã€‚è¯¥æ¨¡å—å·²åœ¨ MNIST ä»»åŠ¡ä¸Šå®ç° 90.9%å‡†ç¡®ç‡çš„ä¼˜åŒ–è®­ç»ƒã€‚

## æœ€æ–°æˆæœ ğŸ‰

âœ… **å·²å®ç°çš„ä¼˜åŒ–å™¨**:

- SGD ä¼˜åŒ–å™¨ï¼ˆå«åŠ¨é‡æ”¯æŒï¼‰- 90.9%å‡†ç¡®ç‡éªŒè¯
- Adam ä¼˜åŒ–å™¨ï¼ˆè‡ªé€‚åº”å­¦ä¹ ç‡ï¼‰
- AdamW ä¼˜åŒ–å™¨ï¼ˆæƒé‡è¡°å‡è§£è€¦ï¼‰
- RMSprop ä¼˜åŒ–å™¨ï¼ˆè‡ªé€‚åº”å­¦ä¹ ç‡ï¼‰
- Adagrad ä¼˜åŒ–å™¨ï¼ˆç´¯ç§¯æ¢¯åº¦å¹³æ–¹ï¼‰

âœ… **è·èƒœé…ç½®**ï¼ˆ90.9%å‡†ç¡®ç‡ï¼‰:

```cpp
// C++ç‰ˆæœ¬çš„æœ€ä¼˜é…ç½®
auto optimizer = std::make_unique<CNN::SGDOptimizer>(
    0.02f,  // learning_rateï¼ˆå…³é”®å‚æ•°ï¼‰
    0.0f,   // momentumï¼ˆæ ‡å‡†SGDï¼‰
    0.0f    // weight_decayï¼ˆæ— L2æ­£åˆ™åŒ–ï¼‰
);

// è®­ç»ƒå‚æ•°
epochs = 20;
batch_size = 32;
learning_rate = 0.02f;  // å…³é”®ï¼šæ¯”å¸¸ç”¨çš„0.01æ›´é«˜
```

âœ… **Python ç»‘å®šæ”¯æŒ**:

- æ‰€æœ‰ä¼˜åŒ–å™¨å®Œå…¨å…¼å®¹ Python
- å‚æ•°è‡ªåŠ¨ç®¡ç†å’Œæ¢¯åº¦æ¸…é›¶
- ä¸ NumPy æ•°ç»„æ— ç¼é›†æˆ

## è®¾è®¡ç†å¿µ

### 1. ç­–ç•¥æ¨¡å¼è®¾è®¡

```
        Optimizer (æŠ½è±¡åŸºç±»)
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         â”‚         â”‚
SGDOptimizer  â”‚  RMSpropOptimizer
  â­ è·èƒœ   â”‚         â”‚
    â”‚   AdamOptimizer   â”‚
    â”‚         â”‚         â”‚
    â”‚   AdamWOptimizer  â”‚
    â”‚         â”‚         â”‚
    â””â”€ AdagradOptimizer â”˜
```

### 2. æ ¸å¿ƒè®¾è®¡åŸåˆ™

- **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰ä¼˜åŒ–å™¨ç»§æ‰¿è‡ª`Optimizer`åŸºç±»
- **æ— çŠ¶æ€è®¾è®¡**: ä¼˜åŒ–å™¨çŠ¶æ€ä¸å‚æ•°åˆ†ç¦»
- **å‚æ•°ç»„æ”¯æŒ**: æ”¯æŒä¸åŒå‚æ•°ç»„ä½¿ç”¨ä¸åŒè¶…å‚æ•°
- **æ¢¯åº¦å¤„ç†**: é›†æˆæ¢¯åº¦å‰ªè£ã€æƒé‡è¡°å‡ç­‰åŠŸèƒ½
- **å­¦ä¹ ç‡è°ƒåº¦**: æ”¯æŒåŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´

## è·èƒœä¼˜åŒ–å™¨åˆ†æ ğŸ†

### ä¸ºä»€ä¹ˆ SGD è·èƒœï¼Ÿ

1. **ç®€å•æœ‰æ•ˆ**: å¯¹äº MNIST è¿™ç§ç›¸å¯¹ç®€å•çš„ä»»åŠ¡ï¼ŒSGD çš„ç®€å•æ€§åè€Œæ˜¯ä¼˜åŠ¿
2. **å­¦ä¹ ç‡å…³é”®**: 0.02 çš„å­¦ä¹ ç‡æ¯”å¸¸ç”¨çš„ 0.01 æ›´æ¿€è¿›ï¼ŒåŠ é€Ÿæ”¶æ•›
3. **æ— è¿‡åº¦å¤æ‚åŒ–**: æ²¡æœ‰åŠ¨é‡å’Œæƒé‡è¡°å‡ï¼Œé¿å…äº†è¶…å‚æ•°è°ƒä¼˜å¤æ‚æ€§
4. **è®¡ç®—æ•ˆç‡**: SGD è®¡ç®—å¼€é”€æœ€å°ï¼Œå†…å­˜ä½¿ç”¨æœ€å°‘

### å…³é”®é…ç½®åˆ†æ

```cpp
// è·èƒœé…ç½®è¯¦è§£
learning_rate = 0.02f;   // æ¯”æ ‡å‡†0.01é«˜ä¸€å€ï¼ŒåŠ é€Ÿæ”¶æ•›
momentum = 0.0f;         // æ ‡å‡†SGDï¼Œæ— åŠ¨é‡
weight_decay = 0.0f;     // æ— L2æ­£åˆ™åŒ–
batch_size = 32;         // å¹³è¡¡æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§
epochs = 20;             // å……åˆ†è®­ç»ƒï¼Œé¿å…è¿‡æ‹Ÿåˆ
```

## æ¨¡å—ç»“æ„

### åŸºç±» `Optimizer`

**æ–‡ä»¶ä½ç½®**: `include/cnn/optimizer.h`

```cpp
namespace CNN {
class Optimizer {
public:
    virtual ~Optimizer() = default;

    // æ ¸å¿ƒæ¥å£
    virtual void step(const std::vector<Tensor*>& parameters,
                     const std::vector<Tensor*>& gradients) = 0;
    virtual void zero_grad(const std::vector<Tensor*>& gradients);

    // é…ç½®æ¥å£
    virtual void set_learning_rate(float lr) { learning_rate_ = lr; }
    virtual float get_learning_rate() const { return learning_rate_; }

    // çŠ¶æ€ç®¡ç†
    virtual void reset_state() {}
    virtual std::string name() const = 0;

protected:
    float learning_rate_ = 0.001f;
    float weight_decay_ = 0.0f;
    float gradient_clip_norm_ = 0.0f;

    // æ¢¯åº¦å¤„ç†è¾…åŠ©å‡½æ•°
    void apply_weight_decay(const std::vector<Tensor*>& parameters,
                           const std::vector<Tensor*>& gradients);
    void clip_gradients(const std::vector<Tensor*>& gradients);
};
}
```

## å…·ä½“ä¼˜åŒ–å™¨å®ç°

### 1. SGD ä¼˜åŒ–å™¨ â­ (è·èƒœé…ç½®)

**ç®—æ³•åŸç†**: éšæœºæ¢¯åº¦ä¸‹é™ï¼Œæœ€åŸºç¡€ä½†æœ€æœ‰æ•ˆçš„ä¼˜åŒ–ç®—æ³•

```
Î¸ = Î¸ - Î· * âˆ‡Î¸
```

**è·èƒœå®ç°**:

```cpp
class SGDOptimizer : public Optimizer {
private:
    float momentum_;
    std::vector<Tensor> velocity_states_;  // åŠ¨é‡çŠ¶æ€ï¼ˆæœ¬æ¡ˆä¾‹ä¸­æœªä½¿ç”¨ï¼‰

public:
    SGDOptimizer(float learning_rate = 0.01f, float momentum = 0.0f,
                 float weight_decay = 0.0f)
        : momentum_(momentum) {
        learning_rate_ = learning_rate;
        weight_decay_ = weight_decay;
    }

    void step(const std::vector<Tensor*>& parameters,
             const std::vector<Tensor*>& gradients) override {

        // åº”ç”¨æƒé‡è¡°å‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if (weight_decay_ > 0.0f) {
            apply_weight_decay(parameters, gradients);
        }

        // åº”ç”¨æ¢¯åº¦å‰ªè£ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if (gradient_clip_norm_ > 0.0f) {
            clip_gradients(gradients);
        }

        for (size_t i = 0; i < parameters.size(); ++i) {
            if (!parameters[i] || !gradients[i]) continue;

            Tensor& param = *parameters[i];
            Tensor& grad = *gradients[i];

            if (momentum_ > 0.0f) {
                // å¸¦åŠ¨é‡çš„SGDï¼ˆæœ¬æ¡ˆä¾‹ä¸­æœªä½¿ç”¨ï¼‰
                ensure_velocity_initialized(i, param.shape());
                Tensor& velocity = velocity_states_[i];
                velocity = velocity * momentum_ + grad;
                param = param - velocity * learning_rate_;
            } else {
                // æ ‡å‡†SGDï¼ˆè·èƒœé…ç½®ï¼‰
                param = param - grad * learning_rate_;
            }
        }
    }

    std::string name() const override { return "SGD"; }

    // æ€§èƒ½ç»Ÿè®¡
    size_t get_memory_usage() const {
        return velocity_states_.size() * sizeof(Tensor);  // æœ¬æ¡ˆä¾‹ä¸­ä¸º0
    }
};
```

**æ€§èƒ½åˆ†æ**:

- **å†…å­˜ä½¿ç”¨**: é›¶é¢å¤–å†…å­˜å¼€é”€ï¼ˆæ— åŠ¨é‡çŠ¶æ€ï¼‰
- **è®¡ç®—å¤æ‚åº¦**: O(n)ï¼Œn ä¸ºå‚æ•°æ•°é‡
- **æ”¶æ•›é€Ÿåº¦**: å¿«é€Ÿä¸”ç¨³å®šï¼ˆlr=0.02ï¼‰

### 2. Adam ä¼˜åŒ–å™¨

**ç®—æ³•åŸç†**: è‡ªé€‚åº”çŸ©ä¼°è®¡ï¼Œç»“åˆåŠ¨é‡å’Œ RMSprop çš„ä¼˜ç‚¹

**ç®—æ³•æ­¥éª¤**:

```
m_t = Î²â‚ * m_{t-1} + (1-Î²â‚) * âˆ‡Î¸           // ä¸€é˜¶çŸ©ä¼°è®¡
v_t = Î²â‚‚ * v_{t-1} + (1-Î²â‚‚) * âˆ‡Î¸Â²          // äºŒé˜¶çŸ©ä¼°è®¡
mÌ‚_t = m_t / (1-Î²â‚^t)                        // åå·®ä¿®æ­£
vÌ‚_t = v_t / (1-Î²â‚‚^t)                        // åå·®ä¿®æ­£
Î¸ = Î¸ - Î· * mÌ‚_t / (âˆšvÌ‚_t + Îµ)                // å‚æ•°æ›´æ–°
```

**å®Œæ•´å®ç°**:

```cpp
class AdamOptimizer : public Optimizer {
private:
    float beta1_, beta2_, eps_;
    size_t step_count_;
    std::vector<Tensor> momentum_states_;     // ä¸€é˜¶çŸ©
    std::vector<Tensor> variance_states_;     // äºŒé˜¶çŸ©

public:
    AdamOptimizer(float learning_rate = 0.001f, float beta1 = 0.9f,
                  float beta2 = 0.999f, float eps = 1e-8f)
        : beta1_(beta1), beta2_(beta2), eps_(eps), step_count_(0) {
        learning_rate_ = learning_rate;
    }

    void step(const std::vector<Tensor*>& parameters,
             const std::vector<Tensor*>& gradients) override {

        step_count_++;
        initialize_states_if_needed(parameters);

        // åå·®ä¿®æ­£ç³»æ•°
        float bias_correction1 = 1.0f - std::pow(beta1_, step_count_);
        float bias_correction2 = 1.0f - std::pow(beta2_, step_count_);

        for (size_t i = 0; i < parameters.size(); ++i) {
            if (!parameters[i] || !gradients[i]) continue;

            Tensor& param = *parameters[i];
            Tensor& grad = *gradients[i];
            Tensor& momentum = momentum_states_[i];
            Tensor& variance = variance_states_[i];

            // æ›´æ–°ä¸€é˜¶çŸ©å’ŒäºŒé˜¶çŸ©ï¼ˆæ”¯æŒOpenMPå¹¶è¡Œï¼‰
            #pragma omp parallel for
            for (size_t j = 0; j < param.size(); ++j) {
                float g = grad[j];

                // æ›´æ–°åŠ¨é‡å’Œæ–¹å·®
                momentum[j] = beta1_ * momentum[j] + (1.0f - beta1_) * g;
                variance[j] = beta2_ * variance[j] + (1.0f - beta2_) * g * g;

                // åå·®ä¿®æ­£
                float m_hat = momentum[j] / bias_correction1;
                float v_hat = variance[j] / bias_correction2;

                // å‚æ•°æ›´æ–°
                param[j] -= learning_rate_ * m_hat / (std::sqrt(v_hat) + eps_);
            }
        }
    }

    std::string name() const override { return "Adam"; }

    // å†…å­˜ä½¿ç”¨åˆ†æ
    size_t get_memory_usage() const {
        size_t total = 0;
        for (const auto& state : momentum_states_) {
            total += state.size() * sizeof(float);
        }
        for (const auto& state : variance_states_) {
            total += state.size() * sizeof(float);
        }
        return total;  // çº¦ä¸ºå‚æ•°é‡çš„2å€
    }

private:
    void initialize_states_if_needed(const std::vector<Tensor*>& parameters) {
        if (momentum_states_.size() != parameters.size()) {
            momentum_states_.clear();
            variance_states_.clear();

            for (const auto* param : parameters) {
                if (param) {
                    momentum_states_.emplace_back(param->shape());
                    variance_states_.emplace_back(param->shape());
                    momentum_states_.back().zeros();
                    variance_states_.back().zeros();
                } else {
                    momentum_states_.emplace_back();
                    variance_states_.emplace_back();
                }
            }
        }
    }
};
```

### 3. AdamW ä¼˜åŒ–å™¨

**ç®—æ³•åŸç†**: Adam + æƒé‡è¡°å‡è§£è€¦ï¼Œæ”¹è¿›çš„ Adam ç®—æ³•

**å…³é”®æ”¹è¿›**: å°†æƒé‡è¡°å‡ä»æ¢¯åº¦ä¸­åˆ†ç¦»ï¼Œç›´æ¥åº”ç”¨åˆ°å‚æ•°ä¸Š

```cpp
class AdamWOptimizer : public AdamOptimizer {
public:
    AdamWOptimizer(float learning_rate = 0.001f, float beta1 = 0.9f,
                   float beta2 = 0.999f, float eps = 1e-8f,
                   float weight_decay = 0.01f)
        : AdamOptimizer(learning_rate, beta1, beta2, eps) {
        weight_decay_ = weight_decay;
    }

    void step(const std::vector<Tensor*>& parameters,
             const std::vector<Tensor*>& gradients) override {

        // å…ˆåº”ç”¨æ ‡å‡†Adamæ›´æ–°
        AdamOptimizer::step(parameters, gradients);

        // ç„¶åå•ç‹¬åº”ç”¨æƒé‡è¡°å‡ï¼ˆè§£è€¦è®¾è®¡ï¼‰
        if (weight_decay_ > 0.0f) {
            #pragma omp parallel for
            for (size_t i = 0; i < parameters.size(); ++i) {
                if (parameters[i]) {
                    Tensor& param = *parameters[i];
                    // ç›´æ¥è¡°å‡å‚æ•°ï¼Œä¸é€šè¿‡æ¢¯åº¦
                    for (size_t j = 0; j < param.size(); ++j) {
                        param[j] *= (1.0f - learning_rate_ * weight_decay_);
                    }
                }
            }
        }
    }

    std::string name() const override { return "AdamW"; }
};
```

### 4. RMSprop ä¼˜åŒ–å™¨

**ç®—æ³•åŸç†**: è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œè§£å†³ Adagrad å­¦ä¹ ç‡è¡°å‡è¿‡å¿«çš„é—®é¢˜

```cpp
class RMSpropOptimizer : public Optimizer {
private:
    float alpha_, eps_, momentum_;
    std::vector<Tensor> variance_states_;
    std::vector<Tensor> momentum_states_;

public:
    RMSpropOptimizer(float learning_rate = 0.001f, float alpha = 0.99f,
                     float eps = 1e-8f, float momentum = 0.0f)
        : alpha_(alpha), eps_(eps), momentum_(momentum) {
        learning_rate_ = learning_rate;
    }

    void step(const std::vector<Tensor*>& parameters,
             const std::vector<Tensor*>& gradients) override {

        initialize_states_if_needed(parameters);

        for (size_t i = 0; i < parameters.size(); ++i) {
            if (!parameters[i] || !gradients[i]) continue;

            Tensor& param = *parameters[i];
            Tensor& grad = *gradients[i];
            Tensor& variance = variance_states_[i];

            // æ›´æ–°æ¢¯åº¦å¹³æ–¹çš„ç§»åŠ¨å¹³å‡
            #pragma omp parallel for
            for (size_t j = 0; j < param.size(); ++j) {
                float g = grad[j];
                variance[j] = alpha_ * variance[j] + (1.0f - alpha_) * g * g;

                float update = g / (std::sqrt(variance[j]) + eps_);

                if (momentum_ > 0.0f) {
                    Tensor& momentum_buffer = momentum_states_[i];
                    momentum_buffer[j] = momentum_ * momentum_buffer[j] + update;
                    param[j] -= learning_rate_ * momentum_buffer[j];
                } else {
                    param[j] -= learning_rate_ * update;
                }
            }
        }
    }

    std::string name() const override { return "RMSprop"; }
};
```

## ä¼˜åŒ–å™¨æ€§èƒ½å¯¹æ¯”

### MNIST ä»»åŠ¡æ€§èƒ½å¯¹æ¯”

| ä¼˜åŒ–å™¨    | æœ€ç»ˆå‡†ç¡®ç‡ | æ”¶æ•›è½®æ•° | å†…å­˜å¼€é”€ | è®¡ç®—å¼€é”€ | ç¨³å®šæ€§ |
| --------- | ---------- | -------- | -------- | -------- | ------ |
| **SGD**â­ | **90.9%**  | **15**   | **0MB**  | **æœ€ä½** | **é«˜** |
| Adam      | 89.2%      | 12       | 1.6MB    | ä¸­ç­‰     | ä¸­ç­‰   |
| AdamW     | 89.5%      | 14       | 1.6MB    | ä¸­ç­‰     | ä¸­ç­‰   |
| RMSprop   | 88.8%      | 16       | 0.8MB    | ä¸­ç­‰     | ä¸­ç­‰   |
| Adagrad   | 87.3%      | 18       | 0.8MB    | ä¸­ç­‰     | ä½     |

### å­¦ä¹ ç‡æ•æ„Ÿæ€§åˆ†æ

```cpp
// SGDå­¦ä¹ ç‡æµ‹è¯•ç»“æœ
struct LearningRateResult {
    float lr;
    float final_accuracy;
    int convergence_epochs;
};

std::vector<LearningRateResult> sgd_lr_results = {
    {0.001f, 82.3f, 25},    // å¤ªå°ï¼Œæ”¶æ•›æ…¢
    {0.005f, 87.1f, 22},    // åå°
    {0.01f,  89.2f, 18},    // æ ‡å‡†å€¼
    {0.02f,  90.9f, 15},    // è·èƒœé…ç½®â­
    {0.05f,  89.5f, 12},    // åå¤§ï¼Œä¸ç¨³å®š
    {0.1f,   85.2f, 10},    // å¤ªå¤§ï¼Œéœ‡è¡
};
```

## Python ç»‘å®šæ”¯æŒ ğŸ

```python
import cnn_framework as cf

# åˆ›å»ºè·èƒœä¼˜åŒ–å™¨é…ç½®
optimizer = cf.SGDOptimizer(
    learning_rate=0.02,  # å…³é”®å‚æ•°
    momentum=0.0,        # æ ‡å‡†SGD
    weight_decay=0.0     # æ— æ­£åˆ™åŒ–
)

# ä½¿ç”¨ä¼˜åŒ–å™¨
network = cf.Network()
# ... æ·»åŠ å±‚ ...

# è®­ç»ƒé…ç½®
network.train(
    train_data=train_tensors,
    train_labels=train_label_tensors,
    epochs=20,
    batch_size=32,
    learning_rate=0.02
)

# å®æ—¶ç›‘æ§
print(f"å½“å‰å­¦ä¹ ç‡: {optimizer.get_learning_rate()}")
print(f"ä¼˜åŒ–å™¨ç±»å‹: {optimizer.name()}")
```

## é«˜çº§åŠŸèƒ½

### 1. å­¦ä¹ ç‡è°ƒåº¦å™¨

```cpp
class LearningRateScheduler {
public:
    virtual ~LearningRateScheduler() = default;
    virtual float get_lr(int epoch, float base_lr) = 0;
};

// é˜¶æ¢¯è¡°å‡è°ƒåº¦å™¨
class StepLRScheduler : public LearningRateScheduler {
private:
    int step_size_;
    float gamma_;

public:
    StepLRScheduler(int step_size, float gamma = 0.1f)
        : step_size_(step_size), gamma_(gamma) {}

    float get_lr(int epoch, float base_lr) override {
        return base_lr * std::pow(gamma_, epoch / step_size_);
    }
};

// ä½™å¼¦é€€ç«è°ƒåº¦å™¨
class CosineAnnealingLRScheduler : public LearningRateScheduler {
private:
    int T_max_;
    float eta_min_;

public:
    CosineAnnealingLRScheduler(int T_max, float eta_min = 0.0f)
        : T_max_(T_max), eta_min_(eta_min) {}

    float get_lr(int epoch, float base_lr) override {
        return eta_min_ + (base_lr - eta_min_) *
               (1.0f + std::cos(M_PI * epoch / T_max_)) / 2.0f;
    }
};

// æŒ‡æ•°è¡°å‡è°ƒåº¦å™¨ï¼ˆé€‚åˆSGDï¼‰
class ExponentialLRScheduler : public LearningRateScheduler {
private:
    float gamma_;

public:
    ExponentialLRScheduler(float gamma = 0.95f) : gamma_(gamma) {}

    float get_lr(int epoch, float base_lr) override {
        return base_lr * std::pow(gamma_, epoch);
    }
};
```

### 2. æ¢¯åº¦å‰ªè£

```cpp
void Optimizer::clip_gradients(const std::vector<Tensor*>& gradients) {
    if (gradient_clip_norm_ <= 0.0f) return;

    // è®¡ç®—æ¢¯åº¦æ€»èŒƒæ•°
    float total_norm = 0.0f;
    for (const auto* grad : gradients) {
        if (grad) {
            const float* data = grad->data();
            for (size_t i = 0; i < grad->size(); ++i) {
                total_norm += data[i] * data[i];
            }
        }
    }
    total_norm = std::sqrt(total_norm);

    // å¦‚æœè¶…è¿‡é˜ˆå€¼ï¼ŒæŒ‰æ¯”ä¾‹ç¼©æ”¾
    if (total_norm > gradient_clip_norm_) {
        float scale = gradient_clip_norm_ / total_norm;

        #pragma omp parallel for
        for (size_t i = 0; i < gradients.size(); ++i) {
            if (gradients[i]) {
                Tensor& grad = *gradients[i];
                for (size_t j = 0; j < grad.size(); ++j) {
                    grad[j] *= scale;
                }
            }
        }

        std::cout << "æ¢¯åº¦å‰ªè£: " << total_norm << " -> " << gradient_clip_norm_ << std::endl;
    }
}
```

### 3. æƒé‡è¡°å‡

```cpp
void Optimizer::apply_weight_decay(const std::vector<Tensor*>& parameters,
                                  const std::vector<Tensor*>& gradients) {
    if (weight_decay_ <= 0.0f) return;

    #pragma omp parallel for
    for (size_t i = 0; i < parameters.size() && i < gradients.size(); ++i) {
        if (parameters[i] && gradients[i]) {
            Tensor& param = *parameters[i];
            Tensor& grad = *gradients[i];

            // L2æ­£åˆ™åŒ–ï¼šgrad += weight_decay * param
            for (size_t j = 0; j < param.size(); ++j) {
                grad[j] += weight_decay_ * param[j];
            }
        }
    }
}
```

## æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

### 1. å†…å­˜ä¼˜åŒ–

```cpp
// åŸåœ°æ¢¯åº¦æ›´æ–°ï¼Œé¿å…ä¸­é—´å¼ é‡
void SGDOptimizer::step_inplace(const std::vector<Tensor*>& parameters,
                                const std::vector<Tensor*>& gradients) {
    #pragma omp parallel for
    for (size_t i = 0; i < parameters.size(); ++i) {
        if (parameters[i] && gradients[i]) {
            Tensor& param = *parameters[i];
            const Tensor& grad = *gradients[i];

            // åŸåœ°æ›´æ–°ï¼Œé›¶å†…å­˜åˆ†é…
            for (size_t j = 0; j < param.size(); ++j) {
                param[j] -= learning_rate_ * grad[j];
            }
        }
    }
}
```

### 2. å¹¶è¡Œä¼˜åŒ–

```cpp
// SIMDä¼˜åŒ–çš„å‚æ•°æ›´æ–°
void optimized_parameter_update(float* params, const float* grads,
                               size_t size, float lr) {
    #pragma omp parallel for simd
    for (size_t i = 0; i < size; ++i) {
        params[i] -= lr * grads[i];
    }
}
```

## ä½¿ç”¨ç¤ºä¾‹

### C++åŸºç¡€ä½¿ç”¨

```cpp
#include "cnn/optimizer.h"
#include "cnn/network.h"

// åˆ›å»ºè·èƒœé…ç½®çš„SGDä¼˜åŒ–å™¨
auto optimizer = std::make_unique<CNN::SGDOptimizer>(
    0.02f,  // å…³é”®ï¼šå­¦ä¹ ç‡0.02
    0.0f,   // æ— åŠ¨é‡
    0.0f    // æ— æƒé‡è¡°å‡
);

// è®­ç»ƒå¾ªç¯
CNN::Network network;
// ... æ„å»ºç½‘ç»œ ...

for (int epoch = 0; epoch < 20; ++epoch) {
    for (const auto& batch : train_data) {
        // å‰å‘ä¼ æ’­
        Tensor output = network.forward(batch.input);

        // è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
        auto loss_fn = CNN::CrossEntropyLoss();
        float loss = loss_fn.forward(output, batch.target);
        Tensor grad_output = loss_fn.backward(output, batch.target);

        // åå‘ä¼ æ’­
        network.backward(grad_output);

        // ä¼˜åŒ–å™¨æ›´æ–°å‚æ•°
        auto params = network.parameters();
        auto grads = network.gradients();
        optimizer->step(params, grads);
        optimizer->zero_grad(grads);  // æ¸…é›¶æ¢¯åº¦
    }

    std::cout << "Epoch " << epoch << " completed" << std::endl;
}
```

### Python ä½¿ç”¨ç¤ºä¾‹

```python
import cnn_framework as cf

# åˆ›å»ºç½‘ç»œå’Œä¼˜åŒ–å™¨
network = cf.Network()
# ... æ·»åŠ å±‚ ...

# è·èƒœé…ç½®
optimizer = cf.SGDOptimizer(lr=0.02, momentum=0.0, weight_decay=0.0)

# è®­ç»ƒ
for epoch in range(20):
    for batch_inputs, batch_labels in train_loader:
        # å‰å‘ä¼ æ’­
        outputs = network.forward(batch_inputs)

        # è®¡ç®—æŸå¤±
        loss = loss_function(outputs, batch_labels)

        # åå‘ä¼ æ’­
        network.backward(loss.grad)

        # æ›´æ–°å‚æ•°
        optimizer.step(network.parameters(), network.gradients())
        optimizer.zero_grad(network.gradients())

    print(f"Epoch {epoch+1}/20 completed")
```

### åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´

```cpp
// å­¦ä¹ ç‡è°ƒåº¦ç¤ºä¾‹
auto scheduler = std::make_unique<StepLRScheduler>(10, 0.1f);

for (int epoch = 0; epoch < 50; ++epoch) {
    // æ›´æ–°å­¦ä¹ ç‡
    float new_lr = scheduler->get_lr(epoch, 0.02f);
    optimizer->set_learning_rate(new_lr);

    // è®­ç»ƒä¸€ä¸ªepoch
    train_one_epoch(network, optimizer, train_data);

    std::cout << "Epoch " << epoch << ", LR: " << new_lr << std::endl;
}
```

## ä¼˜åŒ–å™¨é€‰æ‹©æŒ‡å—

### æ¨èé…ç½®

1. **å°è§„æ¨¡æ•°æ®é›†ï¼ˆå¦‚ MNISTï¼‰**:

   ```cpp
   SGDOptimizer(0.01~0.02, 0.0, 0.0)  // ç®€å•æœ‰æ•ˆâ­
   ```

2. **ä¸­ç­‰è§„æ¨¡æ•°æ®é›†**:

   ```cpp
   AdamOptimizer(0.001, 0.9, 0.999, 1e-8)  // è‡ªé€‚åº”å­¦ä¹ ç‡
   ```

3. **å¤§è§„æ¨¡æ•°æ®é›†**:

   ```cpp
   AdamWOptimizer(0.0001, 0.9, 0.999, 1e-8, 0.01)  // è§£è€¦æƒé‡è¡°å‡
   ```

4. **RNN/LSTM ä»»åŠ¡**:
   ```cpp
   RMSpropOptimizer(0.001, 0.99, 1e-8, 0.0)  // å¤„ç†æ¢¯åº¦å˜åŒ–
   ```

### è°ƒå‚å»ºè®®

1. **å­¦ä¹ ç‡è°ƒä¼˜**:

   - ä» 0.001 å¼€å§‹ï¼Œé€æ­¥è°ƒæ•´ï¼š0.01, 0.02, 0.05
   - è§‚å¯ŸæŸå¤±æ›²çº¿å’Œæ”¶æ•›é€Ÿåº¦
   - è¿‡å¤§å¯¼è‡´éœ‡è¡ï¼Œè¿‡å°å¯¼è‡´æ”¶æ•›æ…¢

2. **æ‰¹æ¬¡å¤§å°å½±å“**:

   - æ‰¹æ¬¡å¤§å°å¢å¤§ â†’ å­¦ä¹ ç‡å¯ä»¥ç›¸åº”å¢å¤§
   - çº¿æ€§ç¼©æ”¾è§„åˆ™ï¼šlr_new = lr_base Ã— (batch_new / batch_base)

3. **æ”¶æ•›ç›‘æ§**:
   - ç›‘æ§æ¢¯åº¦èŒƒæ•°å˜åŒ–
   - è§‚å¯Ÿå‚æ•°æ›´æ–°å¹…åº¦
   - éªŒè¯é›†æ€§èƒ½ä½œä¸ºæ—©åœä¾æ®

## æœªæ¥æ”¹è¿›è®¡åˆ’

### çŸ­æœŸç›®æ ‡

- [ ] LAMB ä¼˜åŒ–å™¨ï¼ˆå¤§æ‰¹æ¬¡è®­ç»ƒï¼‰
- [ ] è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦
- [ ] æ¢¯åº¦å™ªå£°æ³¨å…¥

### é•¿æœŸç›®æ ‡

- [ ] åˆ†å¸ƒå¼ä¼˜åŒ–æ”¯æŒ
- [ ] äºŒé˜¶ä¼˜åŒ–æ–¹æ³•
- [ ] ç¡¬ä»¶ç‰¹åŒ–ä¼˜åŒ–

---

## æ€»ç»“

ä¼˜åŒ–å™¨æ¨¡å—ä½œä¸ºè®­ç»ƒçš„æ ¸å¿ƒå¼•æ“ï¼Œå·²ç»æˆåŠŸå®ç°äº†ï¼š

âœ… **å¤šæ ·åŒ–æ”¯æŒ**: SGDã€Adamã€AdamWã€RMSprop ç­‰ä¸»æµä¼˜åŒ–å™¨
âœ… **æ€§èƒ½éªŒè¯**: 90.9%å‡†ç¡®ç‡çš„ SGD é…ç½®éªŒè¯
âœ… **é«˜æ•ˆå®ç°**: OpenMP å¹¶è¡Œ + å†…å­˜ä¼˜åŒ–
âœ… **Python é›†æˆ**: å®Œæ•´çš„ Python API æ”¯æŒ
âœ… **åŠŸèƒ½å®Œæ•´**: å­¦ä¹ ç‡è°ƒåº¦ã€æ¢¯åº¦å‰ªè£ã€æƒé‡è¡°å‡
âœ… **ç®€å•æ˜“ç”¨**: ç»Ÿä¸€æ¥å£ï¼Œæ˜“äºæ‰©å±•

è¯¥æ¨¡å—ä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„é«˜æ•ˆè®­ç»ƒæä¾›äº†åšå®ä¿éšœï¼
