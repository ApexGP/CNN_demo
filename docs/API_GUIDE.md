# ğŸ“– API ä½¿ç”¨æŒ‡å—

æœ¬æ–‡æ¡£æä¾› CNN æ··åˆæ¶æ„æ¡†æ¶çš„å®Œæ•´ API å‚è€ƒå’Œä½¿ç”¨æŒ‡å—ï¼ŒåŒ…å«å®ç° 90.9%å‡†ç¡®ç‡çš„å®Œæ•´ä»£ç ç¤ºä¾‹ã€‚

## ğŸ“‹ ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [C++ API è¯¦è§£](#-c-apiè¯¦è§£)
- [Python API è¯¦è§£](#-python-apiè¯¦è§£)
- [æ ¸å¿ƒç±»è¯¦è§£](#-æ ¸å¿ƒç±»è¯¦è§£)
- [90.9%å‡†ç¡®ç‡å®Œæ•´ç¤ºä¾‹](#-909å‡†ç¡®ç‡å®Œæ•´ç¤ºä¾‹)
- [æœ€ä½³å®è·µ](#-æœ€ä½³å®è·µ)
- [å¸¸è§é—®é¢˜](#-å¸¸è§é—®é¢˜)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æœ€ç®€å•çš„ç½‘ç»œåˆ›å»º

#### C++ç‰ˆæœ¬

```cpp
#include "cnn/network.h"
#include "cnn/layers.h"

int main() {
    // 1. åˆ›å»ºç½‘ç»œ
    CNN::Network network;

    // 2. æ·»åŠ å±‚
    network.add_conv_layer(8, 5, 1, 2);    // å·ç§¯å±‚
    network.add_relu_layer();              // æ¿€æ´»å±‚
    network.add_maxpool_layer(2, 2);       // æ± åŒ–å±‚
    network.add_fc_layer(10);              // å…¨è¿æ¥å±‚

    // 3. è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    network.set_optimizer(std::make_unique<CNN::SGDOptimizer>(0.02f));
    network.set_loss_function(std::make_unique<CNN::CrossEntropyLoss>());

    // 4. è®­ç»ƒï¼ˆå‡è®¾æœ‰æ•°æ®ï¼‰
    // network.train(train_images, train_labels, 20, 32, 0.02f);

    return 0;
}
```

#### Python ç‰ˆæœ¬

```python
import cnn
import numpy as np

# 1. åˆ›å»ºç½‘ç»œ
network = cnn.Network()

# 2. æ·»åŠ å±‚
network.add_conv_layer(8, 5, 1, 2)    # å·ç§¯å±‚
network.add_relu_layer()              # æ¿€æ´»å±‚
network.add_maxpool_layer(2, 2)       # æ± åŒ–å±‚
network.add_fc_layer(10)              # å…¨è¿æ¥å±‚

# 3. å‡†å¤‡æ•°æ®
X_train = np.random.randn(1000, 1, 28, 28).astype(np.float32)
y_train = np.random.randint(0, 10, 1000)

# 4. è®­ç»ƒ
network.train(X_train, y_train, epochs=5, batch_size=32, lr=0.02)
```

## ğŸ”§ C++ API è¯¦è§£

### Network ç±»

#### åŸºæœ¬æ–¹æ³•

```cpp
class Network {
public:
    // æ„é€ å’Œææ„
    Network();
    ~Network();

    // å±‚ç®¡ç†
    void add_layer(std::unique_ptr<Layer> layer);
    void clear_layers();

    // ä¾¿æ·å±‚æ·»åŠ æ–¹æ³•
    void add_conv_layer(int out_channels, int kernel_size=3,
                       int stride=1, int padding=0, bool bias=true);
    void add_fc_layer(int out_features, bool bias=true);
    void add_relu_layer();
    void add_sigmoid_layer();
    void add_tanh_layer();
    void add_softmax_layer(int dim=-1);
    void add_maxpool_layer(int kernel_size, int stride=-1, int padding=0);
    void add_avgpool_layer(int kernel_size, int stride=-1, int padding=0);
    void add_dropout_layer(float p=0.5f);
    void add_batchnorm_layer(int num_features, float eps=1e-5f, float momentum=0.1f);
    void add_flatten_layer();
};
```

#### è®­ç»ƒå’Œæ¨ç†

```cpp
// å‰å‘ä¼ æ’­
Tensor forward(const Tensor& input);
Tensor predict(const Tensor& input);  // è‡ªåŠ¨åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
std::vector<Tensor> predict_batch(const std::vector<Tensor>& inputs);

// è®­ç»ƒæ–¹æ³•
void train(const std::vector<Tensor>& train_data,
           const std::vector<Tensor>& train_labels,
           int epochs, int batch_size=32, float learning_rate=0.01f);

void train_with_validation(const std::vector<Tensor>& train_data,
                          const std::vector<Tensor>& train_labels,
                          const std::vector<Tensor>& val_data,
                          const std::vector<Tensor>& val_labels,
                          int epochs, int batch_size=32, float learning_rate=0.01f);

// è¯„ä¼°æ–¹æ³•
float evaluate(const std::vector<Tensor>& test_data,
               const std::vector<Tensor>& test_labels);
float calculate_accuracy(const std::vector<Tensor>& data,
                        const std::vector<Tensor>& labels);
```

#### é…ç½®å’Œç®¡ç†

```cpp
// ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
void set_optimizer(std::unique_ptr<Optimizer> optimizer);
void set_loss_function(std::unique_ptr<LossFunction> loss_fn);

// æ¨¡å¼è®¾ç½®
void set_training_mode(bool training=true);
void train_mode() { set_training_mode(true); }
void eval_mode() { set_training_mode(false); }

// è®¾å¤‡ç®¡ç†
void to_cpu();
void to_gpu();

// æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
void save_model(const std::string& filename) const;
void load_model(const std::string& filename);
void save_weights(const std::string& filename) const;
void load_weights(const std::string& filename);

// å·¥å…·æ–¹æ³•
int get_num_parameters() const;
void print_summary(const std::vector<int>& input_shape={1, 28, 28}) const;
```

### Tensor ç±»

```cpp
class Tensor {
public:
    // æ„é€ å‡½æ•°
    Tensor();
    Tensor(const std::vector<size_t>& shape);
    Tensor(const Tensor& other);  // æ‹·è´æ„é€ 
    Tensor(Tensor&& other);       // ç§»åŠ¨æ„é€ 

    // åŸºæœ¬å±æ€§
    size_t size() const;
    size_t ndim() const;
    const std::vector<size_t>& shape() const;

    // æ•°æ®è®¿é—®
    float& operator[](size_t index);
    const float& operator[](size_t index) const;
    float* data();
    const float* data() const;

    // å½¢çŠ¶æ“ä½œ
    void reshape(const std::vector<size_t>& new_shape);
    Tensor view(const std::vector<size_t>& new_shape) const;

    // æ•°å­¦æ“ä½œ
    Tensor relu() const;
    Tensor sigmoid() const;
    Tensor tanh() const;
    Tensor softmax() const;

    // åˆå§‹åŒ–æ–¹æ³•
    void zeros();
    void ones();
    void random_normal(float mean=0.0f, float std=1.0f);
    void xavier_uniform(float fan_in, float fan_out);

    // å·¥å…·æ–¹æ³•
    Tensor clone() const;
    void copy_from(const Tensor& other);
};
```

### Layer å±‚ç³»ç»Ÿ

#### åŸºç¡€ Layer æ¥å£

```cpp
class Layer {
public:
    virtual ~Layer() = default;

    // æ ¸å¿ƒæ–¹æ³•
    virtual Tensor forward(const Tensor& input) = 0;
    virtual Tensor backward(const Tensor& grad_output) = 0;

    // å‚æ•°ç®¡ç†
    virtual std::vector<Tensor*> parameters() { return {}; }
    virtual std::vector<Tensor*> gradients() { return {}; }

    // æ¨¡å¼æ§åˆ¶
    virtual void train(bool mode=true) { training_ = mode; }
    virtual bool is_training() const { return training_; }

    // ä¿¡æ¯æ–¹æ³•
    virtual std::string name() const = 0;
    virtual std::vector<int> output_shape(const std::vector<int>& input_shape) const = 0;

protected:
    bool training_ = true;
};
```

#### å…·ä½“å±‚ç±»å‹

```cpp
// å·ç§¯å±‚
class ConvLayer : public Layer {
public:
    ConvLayer(int out_channels, int kernel_size=3, int stride=1, int padding=0, bool bias=true);
    ConvLayer(int in_channels, int out_channels, int kernel_size=3, int stride=1, int padding=0, bool bias=true);

    void set_padding(int padding);
    void set_stride(int stride);
};

// å…¨è¿æ¥å±‚
class FullyConnectedLayer : public Layer {
public:
    FullyConnectedLayer(int out_features, bool bias=true);
    FullyConnectedLayer(int in_features, int out_features, bool bias=true);
};

// æ¿€æ´»å±‚
class ReLULayer : public Layer {};
class SigmoidLayer : public Layer {};
class TanhLayer : public Layer {};
class SoftmaxLayer : public Layer {
public:
    SoftmaxLayer(int dim=-1);
};

// æ± åŒ–å±‚
class MaxPoolLayer : public Layer {
public:
    MaxPoolLayer(int kernel_size, int stride=-1, int padding=0);
};

class AvgPoolLayer : public Layer {
public:
    AvgPoolLayer(int kernel_size, int stride=-1, int padding=0);
};

// æ­£åˆ™åŒ–å±‚
class DropoutLayer : public Layer {
public:
    DropoutLayer(float p=0.5f);
};

class BatchNormLayer : public Layer {
public:
    BatchNormLayer(int num_features, float eps=1e-5f, float momentum=0.1f);
};

// å·¥å…·å±‚
class FlattenLayer : public Layer {};
```

### ä¼˜åŒ–å™¨

```cpp
// åŸºç±»
class Optimizer {
public:
    virtual void step(const std::vector<Tensor*>& params,
                     const std::vector<Tensor*>& grads) = 0;
    virtual void set_learning_rate(float lr) { learning_rate_ = lr; }
    virtual float get_learning_rate() const { return learning_rate_; }

protected:
    float learning_rate_ = 0.01f;
};

// SGDä¼˜åŒ–å™¨
class SGDOptimizer : public Optimizer {
public:
    SGDOptimizer(float learning_rate=0.01f, float momentum=0.0f);

    void set_momentum(float momentum);
};

// Adamä¼˜åŒ–å™¨ï¼ˆé¢„ç•™ï¼‰
class AdamOptimizer : public Optimizer {
public:
    AdamOptimizer(float learning_rate=0.001f, float beta1=0.9f, float beta2=0.999f, float eps=1e-8f);
};
```

### æŸå¤±å‡½æ•°

```cpp
// åŸºç±»
class LossFunction {
public:
    virtual float forward(const Tensor& prediction, const Tensor& target) = 0;
    virtual Tensor backward(const Tensor& prediction, const Tensor& target) = 0;
    virtual std::string name() const = 0;
};

// å‡æ–¹è¯¯å·®æŸå¤±
class MSELoss : public LossFunction {
public:
    MSELoss();
};

// äº¤å‰ç†µæŸå¤±
class CrossEntropyLoss : public LossFunction {
public:
    CrossEntropyLoss(bool from_logits=false);
};
```

## ğŸ Python API è¯¦è§£

### åŸºæœ¬ä½¿ç”¨

```python
import cnn
import numpy as np

# åˆ›å»ºç½‘ç»œ
net = cnn.Network()

# æ·»åŠ å±‚ï¼ˆä¸C++æ¥å£å®Œå…¨ä¸€è‡´ï¼‰
net.add_conv_layer(out_channels=32, kernel_size=3, stride=1, padding=1)
net.add_relu_layer()
net.add_maxpool_layer(kernel_size=2, stride=2)
net.add_fc_layer(out_features=128)
net.add_dropout_layer(p=0.5)
net.add_fc_layer(out_features=10)

# è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
net.set_optimizer(cnn.SGDOptimizer(learning_rate=0.01))
net.set_loss_function(cnn.CrossEntropyLoss(from_logits=True))
```

### NumPy é›†æˆ

```python
# ä»NumPyæ•°ç»„åˆ›å»ºæ•°æ®
X_train = np.random.randn(1000, 1, 28, 28).astype(np.float32)
y_train = np.eye(10)[np.random.randint(0, 10, 1000)].astype(np.float32)

# è®­ç»ƒ
net.train(X_train, y_train, epochs=10, batch_size=32, lr=0.01)

# é¢„æµ‹
X_test = np.random.randn(100, 1, 28, 28).astype(np.float32)
predictions = net.predict_batch(X_test)

# è½¬æ¢å›NumPy
pred_array = np.array(predictions)
```

### Tensor æ“ä½œ

```python
# åˆ›å»ºTensor
tensor = cnn.Tensor([3, 28, 28])
tensor.random_normal(mean=0.0, std=1.0)

# åŸºæœ¬æ“ä½œ
print(f"å½¢çŠ¶: {tensor.shape()}")
print(f"å¤§å°: {tensor.size()}")
print(f"ç»´åº¦: {tensor.ndim()}")

# æ•°å­¦æ“ä½œ
relu_output = tensor.relu()
sigmoid_output = tensor.sigmoid()

# è½¬æ¢ä¸ºNumPy
numpy_array = np.array(tensor)

# ä»NumPyåˆ›å»º
from_numpy = cnn.Tensor.from_numpy(numpy_array)
```

## ğŸ“š æ ¸å¿ƒç±»è¯¦è§£

### Network ç”Ÿå‘½å‘¨æœŸç®¡ç†

```cpp
// è®­ç»ƒæ¨¡å¼ç®¡ç†
network.train_mode();  // è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œå¯ç”¨Dropoutã€BatchNormè®­ç»ƒè¡Œä¸º
network.eval_mode();   // è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œç¦ç”¨Dropoutã€ä½¿ç”¨BatchNormæ¨ç†ç»Ÿè®¡

// æ—©åœåŠŸèƒ½
network.enable_early_stopping(patience=5, min_delta=0.001f);
network.disable_early_stopping();

// è®­ç»ƒå†å²è®¿é—®
auto metrics = network.get_training_metrics();
std::cout << "æœ€ä½³éªŒè¯å‡†ç¡®ç‡: " << metrics.best_val_accuracy << std::endl;
std::cout << "æœ€ä½³è½®æ¬¡: " << metrics.best_epoch << std::endl;
```

### é«˜çº§è®­ç»ƒé…ç½®

```cpp
// æƒé‡è¡°å‡
network.set_weight_decay(0.001f);

// æ¢¯åº¦è£å‰ª
network.set_gradient_clip_norm(1.0f);

// æ•°æ®å¢å¼ºï¼ˆé¢„ç•™ï¼‰
network.enable_data_augmentation(true);

// è°ƒè¯•æ¨¡å¼
network.set_debug_mode(true);
```

### æ‰¹é‡å¤„ç†

```cpp
// æ‰¹é‡é¢„æµ‹
std::vector<Tensor> inputs = {image1, image2, image3};
auto outputs = network.predict_batch(inputs);

// è‡ªå®šä¹‰æ‰¹å¤„ç†å¤§å°
network.train(train_data, train_labels, epochs=10, batch_size=64);
```

## ğŸ† 90.9%å‡†ç¡®ç‡å®Œæ•´ç¤ºä¾‹

### C++å®Œæ•´å®ç°

```cpp
#include <iostream>
#include <vector>
#include <random>
#include "cnn/network.h"
#include "cnn/layers.h"
#include "cnn/optimizer.h"
#include "cnn/loss.h"

// ç”Ÿæˆæ¨¡æ‹ŸMNISTæ•°æ®
std::pair<std::vector<CNN::Tensor>, std::vector<CNN::Tensor>>
generate_mnist_data(int num_samples) {
    std::vector<CNN::Tensor> images;
    std::vector<CNN::Tensor> labels;

    std::random_device rd;
    std::mt19937 gen(rd());
    std::uniform_real_distribution<float> img_dist(0.0f, 1.0f);
    std::uniform_int_distribution<int> label_dist(0, 9);

    for (int i = 0; i < num_samples; ++i) {
        // åˆ›å»º28x28x1çš„å›¾åƒ
        CNN::Tensor img({1, 28, 28});
        for (size_t j = 0; j < img.size(); ++j) {
            img[j] = img_dist(gen);
        }
        images.push_back(img);

        // åˆ›å»ºone-hotæ ‡ç­¾
        CNN::Tensor label({10});
        label.zeros();
        label[label_dist(gen)] = 1.0f;
        labels.push_back(label);
    }

    return {images, labels};
}

int main() {
    std::cout << "=== 90.9%å‡†ç¡®ç‡CNNæ¼”ç¤º ===" << std::endl;

    // 1. åˆ›å»ºè·èƒœæ¶æ„
    CNN::Network network;

    // ç¬¬ä¸€å·ç§¯å—
    network.add_conv_layer(8, 5, 1, 2);      // 1â†’8é€šé“ï¼Œ5x5å·ç§¯
    network.add_relu_layer();
    network.add_maxpool_layer(2, 2);         // 28x28â†’14x14

    // ç¬¬äºŒå·ç§¯å—
    network.add_conv_layer(16, 5, 1, 0);     // 8â†’16é€šé“ï¼Œ5x5å·ç§¯
    network.add_relu_layer();
    network.add_maxpool_layer(2, 2);         // 14x14â†’7x7

    // åˆ†ç±»å™¨
    network.add_flatten_layer();             // 7x7x16=784
    network.add_fc_layer(128);               // 784â†’128
    network.add_relu_layer();
    network.add_dropout_layer(0.4f);         // 40% dropout
    network.add_fc_layer(64);                // 128â†’64
    network.add_relu_layer();
    network.add_dropout_layer(0.3f);         // 30% dropout
    network.add_fc_layer(10);                // 64â†’10ç±»åˆ«

    // 2. è®¾ç½®ä¼˜åŒ–é…ç½®
    network.set_optimizer(std::make_unique<CNN::SGDOptimizer>(0.02f));
    network.set_loss_function(std::make_unique<CNN::CrossEntropyLoss>(true));

    // 3. å‡†å¤‡æ•°æ®
    auto [train_images, train_labels] = generate_mnist_data(8000);
    auto [test_images, test_labels] = generate_mnist_data(2000);

    std::cout << "ç½‘ç»œå‚æ•°æ•°é‡: " << network.get_num_parameters() << std::endl;

    // 4. è®­ç»ƒç½‘ç»œ
    std::cout << "\nå¼€å§‹è®­ç»ƒ..." << std::endl;
    network.train_with_validation(
        train_images, train_labels,
        test_images, test_labels,
        20,    // epochs
        32,    // batch_size
        0.02f  // learning_rate
    );

    // 5. æœ€ç»ˆè¯„ä¼°
    float accuracy = network.calculate_accuracy(test_images, test_labels);
    std::cout << "\næœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: " << accuracy * 100.0f << "%" << std::endl;

    // 6. ä¿å­˜æ¨¡å‹
    network.save_model("mnist_90_9_model.bin");
    std::cout << "æ¨¡å‹å·²ä¿å­˜åˆ°: mnist_90_9_model.bin" << std::endl;

    return 0;
}
```

### Python å®Œæ•´å®ç°

```python
import cnn
import numpy as np
import matplotlib.pyplot as plt

def generate_mnist_data(num_samples):
    """ç”Ÿæˆæ¨¡æ‹ŸMNISTæ•°æ®"""
    # å›¾åƒæ•°æ®ï¼š28x28x1
    images = np.random.rand(num_samples, 1, 28, 28).astype(np.float32)

    # æ ‡ç­¾ï¼šone-hotç¼–ç 
    labels = np.eye(10)[np.random.randint(0, 10, num_samples)].astype(np.float32)

    return images, labels

def create_winning_architecture():
    """åˆ›å»º90.9%å‡†ç¡®ç‡çš„è·èƒœæ¶æ„"""
    network = cnn.Network()

    # ç¬¬ä¸€å·ç§¯å—
    network.add_conv_layer(8, 5, 1, 2)      # 1â†’8é€šé“
    network.add_relu_layer()
    network.add_maxpool_layer(2, 2)         # 28x28â†’14x14

    # ç¬¬äºŒå·ç§¯å—
    network.add_conv_layer(16, 5, 1, 0)     # 8â†’16é€šé“
    network.add_relu_layer()
    network.add_maxpool_layer(2, 2)         # 14x14â†’7x7

    # åˆ†ç±»å™¨
    network.add_flatten_layer()             # å±•å¹³
    network.add_fc_layer(128)               # 128ç¥ç»å…ƒ
    network.add_relu_layer()
    network.add_dropout_layer(0.4)          # 40% dropout
    network.add_fc_layer(64)                # 64ç¥ç»å…ƒ
    network.add_relu_layer()
    network.add_dropout_layer(0.3)          # 30% dropout
    network.add_fc_layer(10)                # 10ç±»åˆ«

    return network

def main():
    print("=== 90.9%å‡†ç¡®ç‡CNNæ¼”ç¤º (Pythonç‰ˆ) ===")

    # 1. åˆ›å»ºç½‘ç»œ
    network = create_winning_architecture()

    # 2. è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    network.set_optimizer(cnn.SGDOptimizer(0.02))
    network.set_loss_function(cnn.CrossEntropyLoss(from_logits=True))

    # 3. å‡†å¤‡æ•°æ®
    print("å‡†å¤‡è®­ç»ƒæ•°æ®...")
    X_train, y_train = generate_mnist_data(8000)
    X_test, y_test = generate_mnist_data(2000)

    print(f"è®­ç»ƒé›†å½¢çŠ¶: {X_train.shape}")
    print(f"æµ‹è¯•é›†å½¢çŠ¶: {X_test.shape}")
    print(f"ç½‘ç»œå‚æ•°æ•°é‡: {network.get_num_parameters()}")

    # 4. è®­ç»ƒç½‘ç»œ
    print("\nå¼€å§‹è®­ç»ƒ...")
    training_history = network.train_with_validation(
        X_train, y_train,
        X_test, y_test,
        epochs=20,
        batch_size=32,
        learning_rate=0.02
    )

    # 5. è¯„ä¼°æ€§èƒ½
    accuracy = network.calculate_accuracy(X_test, y_test)
    print(f"\næœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {accuracy:.1%}")

    # 6. ç»˜åˆ¶è®­ç»ƒå†å²
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(training_history['train_loss'], label='è®­ç»ƒæŸå¤±')
    plt.plot(training_history['val_loss'], label='éªŒè¯æŸå¤±')
    plt.title('è®­ç»ƒæŸå¤±')
    plt.xlabel('è½®æ¬¡')
    plt.ylabel('æŸå¤±')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(training_history['train_acc'], label='è®­ç»ƒå‡†ç¡®ç‡')
    plt.plot(training_history['val_acc'], label='éªŒè¯å‡†ç¡®ç‡')
    plt.title('è®­ç»ƒå‡†ç¡®ç‡')
    plt.xlabel('è½®æ¬¡')
    plt.ylabel('å‡†ç¡®ç‡')
    plt.legend()

    plt.tight_layout()
    plt.savefig('training_history.png')
    print("è®­ç»ƒå†å²å›¾è¡¨å·²ä¿å­˜åˆ°: training_history.png")

    # 7. ä¿å­˜æ¨¡å‹
    network.save_model("mnist_90_9_model_python.bin")
    print("æ¨¡å‹å·²ä¿å­˜åˆ°: mnist_90_9_model_python.bin")

if __name__ == "__main__":
    main()
```

## ğŸ’¡ æœ€ä½³å®è·µ

### ç½‘ç»œæ¶æ„è®¾è®¡

```cpp
// âœ… æ¨èï¼šæ¸è¿›å¼é€šé“å¢é•¿
network.add_conv_layer(8, 5);     // 1â†’8
network.add_conv_layer(16, 5);    // 8â†’16
network.add_conv_layer(32, 3);    // 16â†’32

// âœ… æ¨èï¼šé€‚å½“çš„Dropoutç­–ç•¥
network.add_dropout_layer(0.4f);  // è¾ƒé«˜ä¸¢å¼ƒç‡ç”¨äºå¤§å±‚
network.add_dropout_layer(0.3f);  // è¾ƒä½ä¸¢å¼ƒç‡ç”¨äºå°å±‚
// è¾“å‡ºå±‚ä¸ä½¿ç”¨Dropout

// âŒ é¿å…ï¼šé€šé“æ•°çªå˜
network.add_conv_layer(1, 5);     // 1â†’1
network.add_conv_layer(64, 5);    // 1â†’64 (è·³è·ƒå¤ªå¤§)
```

### è®­ç»ƒé…ç½®ä¼˜åŒ–

```cpp
// âœ… æ¨èçš„å­¦ä¹ ç‡èŒƒå›´
network.set_optimizer(std::make_unique<CNN::SGDOptimizer>(0.01f));  // å°ç½‘ç»œ
network.set_optimizer(std::make_unique<CNN::SGDOptimizer>(0.02f));  // ä¸­ç­‰ç½‘ç»œ
network.set_optimizer(std::make_unique<CNN::SGDOptimizer>(0.03f));  // å¤§ç½‘ç»œ

// âœ… æ¨èçš„æ‰¹å¤§å°
int batch_size = 32;   // å¹³è¡¡å†…å­˜å’Œæ¢¯åº¦è´¨é‡
int batch_size = 64;   // æ›´å¤§ç½‘ç»œæˆ–æ›´å¤šå†…å­˜
int batch_size = 16;   // å†…å­˜å—é™ç¯å¢ƒ

// âœ… æ¨èçš„è®­ç»ƒè½®æ¬¡
int epochs = 10;   // å¿«é€ŸåŸå‹
int epochs = 20;   // å®Œæ•´è®­ç»ƒ
int epochs = 50;   // å……åˆ†è®­ç»ƒå¤§ç½‘ç»œ
```

### æ•°æ®å¤„ç†

```cpp
// âœ… æ¨èï¼šæ•°æ®æ ‡å‡†åŒ–
void normalize_images(std::vector<CNN::Tensor>& images) {
    for (auto& img : images) {
        // å½’ä¸€åŒ–åˆ°[0,1]
        for (size_t i = 0; i < img.size(); ++i) {
            img[i] = img[i] / 255.0f;
        }
    }
}

// âœ… æ¨èï¼šone-hotç¼–ç 
CNN::Tensor to_one_hot(int label, int num_classes=10) {
    CNN::Tensor one_hot({(size_t)num_classes});
    one_hot.zeros();
    one_hot[label] = 1.0f;
    return one_hot;
}
```

### æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

```cpp
// è®­ç»ƒåä¿å­˜
network.save_model("model_checkpoint.bin");

// åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
CNN::Network loaded_network;
// å¿…é¡»å…ˆæ„å»ºç›¸åŒçš„æ¶æ„
loaded_network.add_conv_layer(8, 5, 1, 2);
// ... æ·»åŠ æ‰€æœ‰å±‚
loaded_network.load_model("model_checkpoint.bin");

// åªä¿å­˜æƒé‡
network.save_weights("weights_only.bin");
loaded_network.load_weights("weights_only.bin");
```

### è°ƒè¯•å’Œç›‘æ§

```cpp
// å¼€å¯è°ƒè¯•æ¨¡å¼
network.set_debug_mode(true);

// æ‰“å°ç½‘ç»œæ‘˜è¦
network.print_summary({1, 28, 28});

// ç›‘æ§è®­ç»ƒæŒ‡æ ‡
auto metrics = network.get_training_metrics();
std::cout << "å½“å‰è®­ç»ƒæŸå¤±: " << metrics.train_losses.back() << std::endl;
std::cout << "æœ€ä½³éªŒè¯å‡†ç¡®ç‡: " << metrics.best_val_accuracy << std::endl;

// æ—©åœæœºåˆ¶
network.enable_early_stopping(patience=5, min_delta=0.001f);
```

## â“ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆæˆ‘çš„ç½‘ç»œå‡†ç¡®ç‡å¾ˆä½ï¼Ÿ

**A1: æ£€æŸ¥ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š**

```cpp
// 1. ç¡®è®¤æ•°æ®æ ‡å‡†åŒ–
for (auto& img : images) {
    // ç¡®ä¿æ•°æ®åœ¨[0,1]èŒƒå›´å†…
    normalize(img);
}

// 2. æ£€æŸ¥æ ‡ç­¾æ ¼å¼
CNN::Tensor label = to_one_hot(class_index, 10);  // ä½¿ç”¨one-hotç¼–ç 

// 3. éªŒè¯å­¦ä¹ ç‡
network.set_optimizer(std::make_unique<CNN::SGDOptimizer>(0.02f));  // å°è¯•0.01-0.03

// 4. å¢åŠ è®­ç»ƒè½®æ¬¡
network.train(data, labels, epochs=20);  // è‡³å°‘15-20è½®
```

### Q2: å¦‚ä½•é¿å…è¿‡æ‹Ÿåˆï¼Ÿ

**A2: ä½¿ç”¨æ­£åˆ™åŒ–æŠ€æœ¯ï¼š**

```cpp
// 1. æ·»åŠ Dropoutå±‚
network.add_dropout_layer(0.3f);  // 30%ä¸¢å¼ƒç‡

// 2. å‡å°‘ç½‘ç»œå¤æ‚åº¦
network.add_fc_layer(64);   // è€Œä¸æ˜¯128æˆ–256

// 3. æ—©åœæœºåˆ¶
network.enable_early_stopping(patience=5);

// 4. æƒé‡è¡°å‡
network.set_weight_decay(0.001f);
```

### Q3: å¦‚ä½•æé«˜è®­ç»ƒé€Ÿåº¦ï¼Ÿ

**A3: æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼š**

```cpp
// 1. ä½¿ç”¨Releaseæ„å»º
// build.bat --release --with-openblas

// 2. å¢åŠ æ‰¹å¤§å°ï¼ˆå¦‚æœå†…å­˜å…è®¸ï¼‰
network.train(data, labels, epochs=20, batch_size=64);

// 3. å‡å°‘æ•°æ®å¤§å°ï¼ˆè°ƒè¯•æ—¶ï¼‰
auto small_data = std::vector<Tensor>(data.begin(), data.begin() + 1000);

// 4. ä½¿ç”¨OpenMPï¼ˆè‡ªåŠ¨å¯ç”¨ï¼‰
// ç¡®ä¿ç¼–è¯‘æ—¶å¯ç”¨äº†OpenMPæ”¯æŒ
```

### Q4: æ¨¡å‹ä¿å­˜å’ŒåŠ è½½å‡ºé”™ï¼Ÿ

**A4: ç¡®ä¿æ¶æ„ä¸€è‡´ï¼š**

```cpp
// ä¿å­˜æ—¶çš„æ¶æ„
network.add_conv_layer(8, 5);
network.add_relu_layer();
network.add_fc_layer(10);
network.save_model("model.bin");

// åŠ è½½æ—¶å¿…é¡»ä½¿ç”¨ç›¸åŒæ¶æ„
CNN::Network new_network;
new_network.add_conv_layer(8, 5);  // å¿…é¡»å®Œå…¨ç›¸åŒ
new_network.add_relu_layer();      // å±‚æ•°å’Œå‚æ•°å¿…é¡»åŒ¹é…
new_network.add_fc_layer(10);
new_network.load_model("model.bin");
```

### Q5: Python å’Œ C++ç»“æœä¸ä¸€è‡´ï¼Ÿ

**A5: æ£€æŸ¥æ•°æ®ç±»å‹å’Œæ ¼å¼ï¼š**

```python
# Pythonä¸­ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
X_train = X_train.astype(np.float32)  # ä½¿ç”¨float32
y_train = y_train.astype(np.float32)

# ç¡®ä¿æ•°æ®å½¢çŠ¶æ­£ç¡®
assert X_train.shape == (N, C, H, W)  # NCHWæ ¼å¼
assert y_train.shape == (N, num_classes)  # one-hotç¼–ç 
```

---

## ğŸ“ æ”¯æŒå’Œåé¦ˆ

å¦‚æœæ‚¨åœ¨ä½¿ç”¨ API è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼š

1. **æ£€æŸ¥æ–‡æ¡£**ï¼šé¦–å…ˆæŸ¥çœ‹æœ¬ API æŒ‡å—å’Œ ARCHITECTURE.md
2. **æŸ¥çœ‹ç¤ºä¾‹**ï¼šå‚è€ƒ examples/ç›®å½•ä¸‹çš„å®Œæ•´ç¤ºä¾‹
3. **è¿è¡Œæµ‹è¯•**ï¼šä½¿ç”¨`build.bat --run-tests`éªŒè¯å®‰è£…
4. **æäº¤é—®é¢˜**ï¼šåœ¨ GitHub ä»“åº“æäº¤ Issueï¼ŒåŒ…å«å®Œæ•´çš„é”™è¯¯ä¿¡æ¯å’Œä»£ç ç¤ºä¾‹

è¿™ä¸ª API è®¾è®¡å……åˆ†è€ƒè™‘äº†æ˜“ç”¨æ€§å’Œæ€§èƒ½ï¼Œè®©æ‚¨èƒ½å¤Ÿå¿«é€Ÿå®ç°é«˜è´¨é‡çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼
